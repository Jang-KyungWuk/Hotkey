{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d041acbf",
   "metadata": {},
   "source": [
    "# lda 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3827f1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from kiwipiepy import Kiwi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d255f35",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sklda(plaintext, n_top_words=30, n_iter=30):\n",
    "    \"\"\"\n",
    "    ------------------------------------------------------------------------------\n",
    "    \n",
    "    텍스트를 받아 lda로 토픽을 나눕니다.\n",
    "    토픽 수 별로 perplexity를 계산 한 후, perplexity값이 가장 낮은 토픽 수로 분석한 결과를 리스트로 리턴합니다.\n",
    "    \n",
    "    ------------------------------------------------------------------------------\n",
    "    \n",
    "    파라미터 설명\n",
    "    \n",
    "    plaintext : txt, 인스타그램 포스트들이 수집된 원문 텍스트. 'HOTKEY123!@#'로 포스트들을 구분한다.\n",
    "    n_top_words : int, 각 토픽 별로 상위 몇 개의 단어를 리턴할 지\n",
    "    n_iter : int, lda 분석 반복 수\n",
    "    \n",
    "    ------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    \n",
    "# 형태소분석기 키위 인스턴스 생성\n",
    "    kiwi = Kiwi()\n",
    "    kiwi.prepare()\n",
    "      \n",
    "\n",
    "# 전처리함수를 통해 스팸포스트 제거\n",
    "    print(\"\\nFiltering spam post...\")\n",
    "    t0 = time()\n",
    "    t1 = time()\n",
    "    doc = preprocess(plaintext, sep='HOTKEY123!@#', returnPlain=True).replace('#','').split('HOTKEY123!@')\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    print(\"\\nExtracting kiwi features for LDA...\")\n",
    "    t0 = time()\n",
    "\n",
    "# sklearn CountVectorizer의 tokenizer 변수에 넣을 함수 정의\n",
    "    def tokenize_ko(doc):\n",
    "        tokens = kiwi.tokenize(doc)\n",
    "#         추가로 사용해 볼 만한 태그들\n",
    "        tagset = {'VA-I',  'MAG', 'XR', 'NNP', 'NNG'}\n",
    "#         tagset = {'NNP', 'NNG'}\n",
    "        results = []\n",
    "        for token in tokens:\n",
    "            if token.tag in tagset:\n",
    "                results.append(token.form)\n",
    "        return results\n",
    "\n",
    "# sklearn CountVectorizer를 통한 전처리\n",
    "    kiwi_vectorizer = CountVectorizer(min_df=2, max_features=1000, tokenizer=tokenize_ko)\n",
    "    kiwivoca = kiwi_vectorizer.fit_transform(doc)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# sklearn lda 분석을 통해 2~5개의 토픽 수 중 perplexity가 가장 낮은 값 찾기\n",
    "    print(\"\\nFinding the optimal number of topics...\")\n",
    "    t0 = time()\n",
    "    perplexity = []\n",
    "    for i in range(2,6):\n",
    "        lda = LatentDirichletAllocation(\n",
    "            n_components=i,\n",
    "            max_iter=n_iter,\n",
    "            learning_method=\"online\",\n",
    "            learning_offset=50.0,\n",
    "            random_state=0,\n",
    "        )\n",
    "        lda.fit(kiwivoca)\n",
    "        perplexity.append(lda.perplexity(kiwivoca))\n",
    "        \n",
    "# 가장 낮은 perplexity 값을 가지는 최적의 토픽 수로 저장\n",
    "    n_topics=perplexity.index(min(perplexity))+2\n",
    "    print(\"done in %0.3fs.\" % (time() - t0), f\"the optimal number of topics is {n_topics}\")\n",
    "\n",
    "# 최적의 토픽 수로 lda분석\n",
    "    print(\"\\nFitting LDA models with KIWI features, number of topics=%d, max_iter=%d\" % (n_topics, n_iter))\n",
    "    t0 = time()\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics,\n",
    "        max_iter=n_iter,\n",
    "        learning_method=\"online\",\n",
    "        learning_offset=50.0,\n",
    "        random_state=0,\n",
    "    )\n",
    "    lda.fit(kiwivoca)\n",
    "\n",
    "# 토픽 넘버 : 해당 토픽의 토큰들 의 형태로 출력, 같은 토픽의 토큰들로 구성된 리스트 생성\n",
    "    kiwi_feature_names = kiwi_vectorizer.get_feature_names_out()\n",
    "    topic_list = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [kiwi_feature_names[i] for i in top_features_ind]\n",
    "        topic_list.append(top_features)\n",
    "        print('Topic {}: {}'.format(topic_idx+1, ' '.join(top_features))) \n",
    "        \n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "    print(\"in total, %0.3fs.\" % (time() - t1))\n",
    "    return topic_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
