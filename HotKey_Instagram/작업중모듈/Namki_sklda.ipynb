{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d041acbf",
   "metadata": {},
   "source": [
    "# 전처리 이용하지 않는, perplexity 최적화 토픽 수 찾기\n",
    "# 리턴값 = [[토픽1의 토큰들], [토픽2의 토큰들], ... ]\n",
    "# 작동 확인 시 순서대로 실행!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d255f35",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sklda(plaintext, n_top_words=5, n_iter=30):\n",
    "    from time import time\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "    import pickle as pkl\n",
    "    from kiwipiepy import Kiwi\n",
    "    kiwi = Kiwi()\n",
    "    kiwi.prepare()\n",
    "      \n",
    "    n_features = 1000\n",
    "    doc = plaintext.replace('#','').split('HOTKEY123!@')\n",
    "\n",
    "    print(\"\\nExtracting kiwi features for LDA...\")\n",
    "    t0 = time()\n",
    "    t1 = time()\n",
    "\n",
    "    def tokenize_ko(doc):\n",
    "        tokens = kiwi.tokenize(doc)\n",
    "        tagset = {'VA-I',  'MAG', 'XR', 'NNP', 'NNG'}\n",
    "        results = []\n",
    "        for token in tokens:\n",
    "            if token.tag in tagset:\n",
    "                results.append(token.form)\n",
    "        return results\n",
    "    kiwi_vectorizer = CountVectorizer(min_df=2, max_features=n_features, tokenizer=tokenize_ko)\n",
    "    kiwivoca = kiwi_vectorizer.fit_transform(doc)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    print(\"\\nFinding the optimal number of topics...\")\n",
    "    t0 = time()\n",
    "    perplexity = []\n",
    "    for i in range(2,6):\n",
    "        lda = LatentDirichletAllocation(\n",
    "            n_components=i,\n",
    "            max_iter=n_iter,\n",
    "            learning_method=\"online\",\n",
    "            learning_offset=50.0,\n",
    "            random_state=0,\n",
    "        )\n",
    "        lda.fit(kiwivoca)\n",
    "        perplexity.append(lda.perplexity(kiwivoca))\n",
    "    n_topics=perplexity.index(min(perplexity))+2\n",
    "    print(\"done in %0.3fs.\" % (time() - t0), f\"the optimal number of topics is {n_topics}\")\n",
    "\n",
    "    print(\"\\nFitting LDA models with KIWI features, n_features=%d, number of topics=%d, max_iter=%d\" % (n_features, n_topics, n_iter))\n",
    "    t0 = time()\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics,\n",
    "        max_iter=n_iter,\n",
    "        learning_method=\"online\",\n",
    "        learning_offset=50.0,\n",
    "        random_state=0,\n",
    "    )\n",
    "    lda.fit(kiwivoca)\n",
    "\n",
    "    kiwi_feature_names = kiwi_vectorizer.get_feature_names_out()\n",
    "    topic_list = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [kiwi_feature_names[i] for i in top_features_ind]\n",
    "        topic_list.append(top_features)\n",
    "        print('Topic {}: {}'.format(topic_idx+1, ' '.join(top_features))) \n",
    "        \n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    print(\"in total, %0.3fs.\" % (time() - t1))\n",
    "    return topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "637e0ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('월드컵_1203.txt','r',encoding='utf-8') as file:\n",
    "    plaintext = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3f051b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting kiwi features for LDA...\n",
      "done in 1.095s.\n",
      "\n",
      "Finding the optimal number of topics...\n",
      "done in 4.247s. the optimal number of topics is 3\n",
      "\n",
      "Fitting LDA models with KIWI features, n_features=1000, number of topics=3, max_iter=30\n",
      "Topic 1: 다시 붉은악마 기분 날 꿈\n",
      "Topic 2: 월드컵 대한민국 강 진출 축구\n",
      "Topic 3: 월드컵 스타 그램 유머 축구\n",
      "done in 1.039s.\n",
      "in total, 6.380s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['다시', '붉은악마', '기분', '날', '꿈'],\n",
       " ['월드컵', '대한민국', '강', '진출', '축구'],\n",
       " ['월드컵', '스타', '그램', '유머', '축구']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklda(plaintext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92b4756",
   "metadata": {},
   "source": [
    "# 전처리함수 이용하기, 원문반환 추가 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac25380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklda(plaintext, n_top_words=5, n_iter=30):\n",
    "    from time import time\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "    import pickle as pkl\n",
    "    from kiwipiepy import Kiwi\n",
    "    kiwi = Kiwi()\n",
    "    kiwi.prepare()\n",
    "      \n",
    "    n_features = 1000\n",
    "#   원문반환하는 전처리함수 처리 필요\n",
    "    doc = preprocess(plaintext, sep='HOTKEY123!@#', 원문반환)\n",
    "\n",
    "    print(\"\\nExtracting kiwi features for LDA...\")\n",
    "    t0 = time()\n",
    "    t1 = time()\n",
    "\n",
    "    def tokenize_ko(doc):\n",
    "        tokens = kiwi.tokenize(doc)\n",
    "        tagset = {'VA-I',  'MAG', 'XR', 'NNP', 'NNG'}\n",
    "        results = []\n",
    "        for token in tokens:\n",
    "            if token.tag in tagset:\n",
    "                results.append(token.form)\n",
    "        return results\n",
    "    kiwi_vectorizer = CountVectorizer(min_df=2, max_features=n_features, tokenizer=tokenize_ko)\n",
    "    kiwivoca = kiwi_vectorizer.fit_transform(doc)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    print(\"\\nFinding the optimal number of topics...\")\n",
    "    t0 = time()\n",
    "    perplexity = []\n",
    "    for i in range(2,6):\n",
    "        lda = LatentDirichletAllocation(\n",
    "            n_components=i,\n",
    "            max_iter=n_iter,\n",
    "            learning_method=\"online\",\n",
    "            learning_offset=50.0,\n",
    "            random_state=0,\n",
    "        )\n",
    "        lda.fit(kiwivoca)\n",
    "        perplexity.append(lda.perplexity(kiwivoca))\n",
    "    n_topics=perplexity.index(min(perplexity))+2\n",
    "    print(\"done in %0.3fs.\" % (time() - t0), f\"the optimal number of topics is {n_topics}\")\n",
    "\n",
    "    print(\"\\nFitting LDA models with KIWI features, n_features=%d, number of topics=%d, max_iter=%d\" % (n_features, n_topics, n_iter))\n",
    "    t0 = time()\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics,\n",
    "        max_iter=n_iter,\n",
    "        learning_method=\"online\",\n",
    "        learning_offset=50.0,\n",
    "        random_state=0,\n",
    "    )\n",
    "    lda.fit(kiwivoca)\n",
    "\n",
    "    kiwi_feature_names = kiwi_vectorizer.get_feature_names_out()\n",
    "    topic_list = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [kiwi_feature_names[i] for i in top_features_ind]\n",
    "        topic_list.append(top_features)\n",
    "        print('Topic {}: {}'.format(topic_idx+1, ' '.join(top_features))) \n",
    "        \n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    print(\"in total, %0.3fs.\" % (time() - t1))\n",
    "    return topic_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12c3289",
   "metadata": {},
   "source": [
    "# 전처리함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15a39b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "from math import log1p\n",
    "import numpy as np\n",
    "import konlpy\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "def preprocess(plaintext, sep,\n",
    "               morphemeAnalyzer='kiwi', targetMorphs=['NNP','NNG'],removehashtag=True, returnMorph=False,\n",
    "               returnEnglishMorph=False, eeTagRule={'NNP':'NNP'},\n",
    "               filterMorphemeAnalyzer='kiwi',filterTargetMorphs=['NNP','NNG','W_HASHTAG'],\n",
    "               k_1Filter=1.5 ,bFilter=0.75,\n",
    "               filterThreshold = 3.315):\n",
    "    \n",
    "    # 형태소 분석기를 텍스트로 정의 시 해당 형태소 분석기를 할당\n",
    "    tma = set_morpheme_analyzer(morphemeAnalyzer)\n",
    "    fma = set_morpheme_analyzer(filterMorphemeAnalyzer)\n",
    "    \n",
    "    # 서버에서 \"글1 구분자 글2 구분자\" 형식의 데이터를 받는다고 가정\n",
    "    data = plaintext.split(sep)\n",
    "    # 구분자로 데이터를 받으면 글이 존재하지 않는 마지막 부분을 삭제\n",
    "    \n",
    "    if data[-1] in ('',' ','\\n',[],['\\n'],[' ']):\n",
    "        data=data[:-1]\n",
    "    \n",
    "    if removehashtag == True:\n",
    "        if '#' in sep:\n",
    "            sep = sep.replace('#','')\n",
    "        tdata = plaintext.replace('#',' ').split(sep)\n",
    "        if tdata[-1] in ('',' ','\\n',[],['\\n'],[' ']):\n",
    "            tdata=tdata[:-1]\n",
    "    else:\n",
    "        tdata = data*1\n",
    "            \n",
    "    postLens = list()\n",
    "    for post in data:\n",
    "        postLens.append(len(post))\n",
    "\n",
    "    flag = (morphemeAnalyzer==filterMorphemeAnalyzer, set(targetMorphs)==set(filterTargetMorphs))\n",
    "    \n",
    "    \n",
    "    ftok = data_tokenize(data,fma,filterTargetMorphs,\n",
    "                         returnMorph=False,\n",
    "                         returnEnglishMorph=True,\n",
    "                         eeTagRule={'W_HASHTAG':'W_HASHTAG'})\n",
    "    ttok = data_tokenize(tdata,tma,targetMorphs,\n",
    "                         returnMorph=returnMorph,\n",
    "                         returnEnglishMorph=returnEnglishMorph, eeTagRule=eeTagRule)\n",
    "    \n",
    "    filterScores = BM25(ftok, postLens, k_1=k_1Filter, b=bFilter)\n",
    "    \n",
    "    spamCount=0\n",
    "    for idx in range(len(filterScores)):\n",
    "        if filterScores[idx] < filterThreshold:\n",
    "            spamCount+=1\n",
    "            ttok.pop(idx-spamCount)\n",
    "    print(\"%s 개의 데이터가 삭제되었습니다.\"%spamCount)\n",
    "    \n",
    "    return ttok\n",
    "\n",
    "def set_morpheme_analyzer(maText):\n",
    "    if maText in ['kiwi','Kiwi','KIWI','키위']:\n",
    "        return Kiwi()\n",
    "    elif maText in ['Hannanum', 'hannanum', 'HANNANUM','한나눔']:\n",
    "        return konlpy.tag.Hannanum()\n",
    "    elif maText in ['Komoran','KOMORAN','komoran','코모란']:\n",
    "        return konlpy.tag.Komoran()\n",
    "    elif maText in ['Kkma','KKMA','kkma','꼬꼬마']:\n",
    "        return konlpy.tag.Kkma()\n",
    "    elif maText in ['Okt','OKT','okt','오픈코리안텍스트','트위터']:\n",
    "        return konlpy.tag.Okt()\n",
    "    elif maText in ['Mecab','mecab','MECAB','미캐브']:\n",
    "        return konlpy.tag.Mecab()\n",
    "    else:\n",
    "        raise Exception('No such morpheme analyzer\\nSupported morpheme analyzers are Kiwi, KoNLPy(Hannanum, Komoran, Kkma, Okt, Mecab)')\n",
    "        \n",
    "def data_tokenize(data,morphemeAnalyzer,targetMorph,\n",
    "                  returnMorph=False,\n",
    "                  returnEnglishMorph=False,\n",
    "                  eeTagRule={'NNP':'NNP'}):\n",
    "    \n",
    "    returnData = list()\n",
    "    \n",
    "    maDir = dir(morphemeAnalyzer)\n",
    "    if 'pos' in maDir:\n",
    "        for post in data:\n",
    "            selected = list()\n",
    "            if returnEnglishMorph==True:\n",
    "                eeTags, post = emoji_english_preprocess(post, tagRule=eeTagRule, returnMorph=returnMorph)\n",
    "                selected += eeTags\n",
    "            \n",
    "            for tok in morphemeAnalyzer.pos(post):\n",
    "                if tok[1] in targetMorph:\n",
    "                    if returnMorph==True:\n",
    "                        selected.append((tok[0],tok[1]))\n",
    "                    else:\n",
    "                        selected.append(tok[0])\n",
    "            returnData.append(selected)\n",
    "    \n",
    "    elif 'tokenize' in maDir:\n",
    "        for post in data:\n",
    "            selected = list()\n",
    "            if returnEnglishMorph==True:\n",
    "                eeTags, post = emoji_english_preprocess(post, tagRule=eeTagRule, returnMorph=returnMorph)\n",
    "                selected += eeTags\n",
    "            \n",
    "            for tok in morphemeAnalyzer.tokenize(post):\n",
    "                if tok.tag in targetMorph:\n",
    "                    if returnMorph==True:\n",
    "                        selected.append((tok.form,tok.tag))\n",
    "                    else:\n",
    "                        selected.append(tok.form)\n",
    "            returnData.append(selected)\n",
    "    else:\n",
    "        raise Exception('Not supported morpheme analyzer instance')\n",
    "    return returnData\n",
    "\n",
    "def BM25(data, postLens, k_1=1.5, b=0.75):\n",
    "    avgPostLen = np.mean(postLens)\n",
    "    \n",
    "    N = len(data)\n",
    "    \n",
    "    n = dict()\n",
    "    for post in data:\n",
    "        uniqueToks = set(post)\n",
    "        for tok in uniqueToks:\n",
    "            try:\n",
    "                n[tok]+=1\n",
    "            except:\n",
    "                n[tok] = 1\n",
    "    \n",
    "    IDF = dict()\n",
    "    for tok in n.keys():\n",
    "        IDF[tok] = log1p((N-n[tok]+0.5)/(n[tok]+0.5))\n",
    "\n",
    "\n",
    "    filterScores = list()\n",
    "\n",
    "    for postidx, post in enumerate(data):\n",
    "        postScore = 0\n",
    "        for tok in post:\n",
    "            tokCount = post.count(tok)\n",
    "            postScore += (IDF[tok] * (\n",
    "                (tokCount*(k_1+1))/(\n",
    "                    tokCount+(k_1*(1-b+(b*(postLens[postidx]/avgPostLen)))))))\n",
    "        try:\n",
    "            filterScores.append((postScore/len(post)))\n",
    "        except:\n",
    "            filterScores.append(0)\n",
    "\n",
    "    return filterScores\n",
    "\n",
    "def emoji_english_preprocess(post, tagRule={'NNP':'NNP'}, returnMorph=True):\n",
    "    returnData = list()\n",
    "    \n",
    "    emojis = re.findall(':[_A-Za-z]+:',post)\n",
    "    for emoji in set(emojis):\n",
    "        emojiCounts = post.count(emoji)\n",
    "        post = post.replace(emoji,'')\n",
    "        returnData+=([(emoji,'EMJ')]*emojiCounts)\n",
    "        \n",
    "    hashTags = re.findall('#[_A-Za-z]',post)\n",
    "    for hashTag in set(hashTags):\n",
    "        hTagCounts = post.count(hashTag)\n",
    "        post = post.replace(hashTag,'')\n",
    "        returnData+=([(hashTag,'W_HASHTAG')]*hTagCounts)\n",
    "\n",
    "    \n",
    "    engChunks = re.findall('[A-Za-z]+[\\' ]?[A-Za-z]+',post)\n",
    "    for engChunk in set(engChunks):\n",
    "        chunkCounts = post.count(engChunk)\n",
    "        post = post.replace(engChunk,'')\n",
    "\n",
    "        targetToken = list()\n",
    "        for token in nltk.pos_tag(nltk.word_tokenize(engChunk)):\n",
    "            if token[1] in tagRule: \n",
    "                targetToken.append((token[0],token[1]))\n",
    "        returnData+=(targetToken*chunkCounts)\n",
    "\n",
    "    filterData = list()\n",
    "    for token in returnData:\n",
    "        if token[1] in tagRule:\n",
    "            if returnMorph==True:\n",
    "                filterData.append((token[0],tagRule[token[1]]))\n",
    "            else:\n",
    "                filterData.append(token[0])\n",
    "        \n",
    "    return filterData, post\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496b2626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
