{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0090243a",
   "metadata": {},
   "source": [
    "# wordcloud 함수 (작동 확인하려면 순서대로 실행!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52c2db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e427312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud(plaintext, filename, wc_backgroundcolor='black', wc_colormap='autumn'):\n",
    "    \"\"\"\n",
    "    ------------------------------------------------------------------------------\n",
    "    \n",
    "    텍스트를 받아 전처리함수를 통해 토큰화 한 후 빈도를 기반으로 wordcloud 그림파일을 생성합니다. \n",
    "    키값으로 토큰, 밸류값으로 빈도 수를 갖는 딕셔너리를 반환합니다.\n",
    "    \n",
    "    ------------------------------------------------------------------------------\n",
    "    \n",
    "    파라미터 설명\n",
    "    \n",
    "    plaintext : 인스타그램 포스트들이 수집된 원문 텍스트. 'HOTKEY123!@#'로 포스트들을 구분한다.\n",
    "    filename : wordcloud 그림파일을 저장할 파일이름.\n",
    "    wc_backgroundcolor : wordcloud 그림파일의 배경색, 기본값은 'black'\n",
    "    wc_colormap : wordcloud 그림파일의 컬러맵(wordcloud 모듈에 저장된), 기본값은 'autumn'\n",
    "    \n",
    "    ------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "# 입력받은 원문을 전처리함수를 통해 토큰화\n",
    "    pt = preprocess(plaintext=plaintext, sep='HOTKEY123!@#', targetMorphs=['NNP', 'NNG'])\n",
    "\n",
    "# 토큰들의 빈도를 dict형태로 저장\n",
    "    voca = dict()\n",
    "    for post in pt:\n",
    "        for term in post:\n",
    "            if term in voca:\n",
    "                voca[term] += 1\n",
    "            else:\n",
    "                voca[term] = 1\n",
    "                \n",
    "# 빈도 순(밸류값)으로 정렬                \n",
    "    voca_sorted = sorted(voca.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 한글로 wordcloud 이미지를 생성하기 위해 글꼴 불러오기\n",
    "    font_path ='malgun.ttf'\n",
    "    \n",
    "# 이미지 파일 읽어오기\n",
    "    im = Image.open('mask_camera.png') \n",
    "    \n",
    "# 이미지 파일 전처리\n",
    "    mask=Image.new(\"RGB\",im.size, (255,255,255))\n",
    "    mask.paste(im)\n",
    "    mask=np.array(mask)\n",
    "    \n",
    "# wordcloud 이미지 생성\n",
    "    wc=WordCloud(background_color='black', colormap=wc_colormap,font_path = font_path, mask = mask)\n",
    "    wc=wc.generate_from_frequencies(voca)\n",
    "    \n",
    "# 입력받은 경로에 저장, 파일이름 중복될 경우 덮어쓰여짐\n",
    "    wc.to_file(filename=f'{filename}')\n",
    "    \n",
    "    return voca_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f9de4",
   "metadata": {},
   "source": [
    "# 전처리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "924a9e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "from math import log1p\n",
    "import numpy as np\n",
    "import konlpy\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "def preprocess(plaintext, sep,\n",
    "               morphemeAnalyzer='kiwi', targetMorphs=['NNP','NNG'],removehashtag=True, returnMorph=False,\n",
    "               returnEnglishMorph=False, eeTagRule={'NNP':'NNP'},\n",
    "               filterMorphemeAnalyzer='kiwi',filterTargetMorphs=['NNP','NNG','W_HASHTAG'],\n",
    "               k_1Filter=1.5 ,bFilter=0.75,\n",
    "               filterThreshold = 3.315):\n",
    "    \n",
    "    # 형태소 분석기를 텍스트로 정의 시 해당 형태소 분석기를 할당\n",
    "    tma = set_morpheme_analyzer(morphemeAnalyzer)\n",
    "    fma = set_morpheme_analyzer(filterMorphemeAnalyzer)\n",
    "    \n",
    "    # 서버에서 \"글1 구분자 글2 구분자\" 형식의 데이터를 받는다고 가정\n",
    "    data = plaintext.split(sep)\n",
    "    # 구분자로 데이터를 받으면 글이 존재하지 않는 마지막 부분을 삭제\n",
    "    \n",
    "    if data[-1] in ('',' ','\\n',[],['\\n'],[' ']):\n",
    "        data=data[:-1]\n",
    "    \n",
    "    if removehashtag == True:\n",
    "        if '#' in sep:\n",
    "            sep = sep.replace('#','')\n",
    "        tdata = plaintext.replace('#',' ').split(sep)\n",
    "        if tdata[-1] in ('',' ','\\n',[],['\\n'],[' ']):\n",
    "            tdata=tdata[:-1]\n",
    "    else:\n",
    "        tdata = data*1\n",
    "            \n",
    "    postLens = list()\n",
    "    for post in data:\n",
    "        postLens.append(len(post))\n",
    "\n",
    "    flag = (morphemeAnalyzer==filterMorphemeAnalyzer, set(targetMorphs)==set(filterTargetMorphs))\n",
    "    \n",
    "    \n",
    "    ftok = data_tokenize(data,fma,filterTargetMorphs,\n",
    "                         returnMorph=False,\n",
    "                         returnEnglishMorph=True,\n",
    "                         eeTagRule={'W_HASHTAG':'W_HASHTAG'})\n",
    "    ttok = data_tokenize(tdata,tma,targetMorphs,\n",
    "                         returnMorph=returnMorph,\n",
    "                         returnEnglishMorph=returnEnglishMorph, eeTagRule=eeTagRule)\n",
    "    \n",
    "    filterScores = BM25(ftok, postLens, k_1=k_1Filter, b=bFilter)\n",
    "    \n",
    "    spamCount=0\n",
    "    for idx in range(len(filterScores)):\n",
    "        if filterScores[idx] < filterThreshold:\n",
    "            spamCount+=1\n",
    "            ttok.pop(idx-spamCount)\n",
    "    print(\"%s 개의 데이터가 삭제되었습니다.\"%spamCount)\n",
    "    \n",
    "    return ttok\n",
    "\n",
    "def set_morpheme_analyzer(maText):\n",
    "    if maText in ['kiwi','Kiwi','KIWI','키위']:\n",
    "        return Kiwi()\n",
    "    elif maText in ['Hannanum', 'hannanum', 'HANNANUM','한나눔']:\n",
    "        return konlpy.tag.Hannanum()\n",
    "    elif maText in ['Komoran','KOMORAN','komoran','코모란']:\n",
    "        return konlpy.tag.Komoran()\n",
    "    elif maText in ['Kkma','KKMA','kkma','꼬꼬마']:\n",
    "        return konlpy.tag.Kkma()\n",
    "    elif maText in ['Okt','OKT','okt','오픈코리안텍스트','트위터']:\n",
    "        return konlpy.tag.Okt()\n",
    "    elif maText in ['Mecab','mecab','MECAB','미캐브']:\n",
    "        return konlpy.tag.Mecab()\n",
    "    else:\n",
    "        raise Exception('No such morpheme analyzer\\nSupported morpheme analyzers are Kiwi, KoNLPy(Hannanum, Komoran, Kkma, Okt, Mecab)')\n",
    "        \n",
    "def data_tokenize(data,morphemeAnalyzer,targetMorph,\n",
    "                  returnMorph=False,\n",
    "                  returnEnglishMorph=False,\n",
    "                  eeTagRule={'NNP':'NNP'}):\n",
    "    \n",
    "    returnData = list()\n",
    "    \n",
    "    maDir = dir(morphemeAnalyzer)\n",
    "    if 'pos' in maDir:\n",
    "        for post in data:\n",
    "            selected = list()\n",
    "            if returnEnglishMorph==True:\n",
    "                eeTags, post = emoji_english_preprocess(post, tagRule=eeTagRule, returnMorph=returnMorph)\n",
    "                selected += eeTags\n",
    "            \n",
    "            for tok in morphemeAnalyzer.pos(post):\n",
    "                if tok[1] in targetMorph:\n",
    "                    if returnMorph==True:\n",
    "                        selected.append((tok[0],tok[1]))\n",
    "                    else:\n",
    "                        selected.append(tok[0])\n",
    "            returnData.append(selected)\n",
    "    \n",
    "    elif 'tokenize' in maDir:\n",
    "        for post in data:\n",
    "            selected = list()\n",
    "            if returnEnglishMorph==True:\n",
    "                eeTags, post = emoji_english_preprocess(post, tagRule=eeTagRule, returnMorph=returnMorph)\n",
    "                selected += eeTags\n",
    "            \n",
    "            for tok in morphemeAnalyzer.tokenize(post):\n",
    "                if tok.tag in targetMorph:\n",
    "                    if returnMorph==True:\n",
    "                        selected.append((tok.form,tok.tag))\n",
    "                    else:\n",
    "                        selected.append(tok.form)\n",
    "            returnData.append(selected)\n",
    "    else:\n",
    "        raise Exception('Not supported morpheme analyzer instance')\n",
    "    return returnData\n",
    "\n",
    "def BM25(data, postLens, k_1=1.5, b=0.75):\n",
    "    avgPostLen = np.mean(postLens)\n",
    "    \n",
    "    N = len(data)\n",
    "    \n",
    "    n = dict()\n",
    "    for post in data:\n",
    "        uniqueToks = set(post)\n",
    "        for tok in uniqueToks:\n",
    "            try:\n",
    "                n[tok]+=1\n",
    "            except:\n",
    "                n[tok] = 1\n",
    "    \n",
    "    IDF = dict()\n",
    "    for tok in n.keys():\n",
    "        IDF[tok] = log1p((N-n[tok]+0.5)/(n[tok]+0.5))\n",
    "\n",
    "\n",
    "    filterScores = list()\n",
    "\n",
    "    for postidx, post in enumerate(data):\n",
    "        postScore = 0\n",
    "        for tok in post:\n",
    "            tokCount = post.count(tok)\n",
    "            postScore += (IDF[tok] * (\n",
    "                (tokCount*(k_1+1))/(\n",
    "                    tokCount+(k_1*(1-b+(b*(postLens[postidx]/avgPostLen)))))))\n",
    "        try:\n",
    "            filterScores.append((postScore/len(post)))\n",
    "        except:\n",
    "            filterScores.append(0)\n",
    "\n",
    "    return filterScores\n",
    "\n",
    "def emoji_english_preprocess(post, tagRule={'NNP':'NNP'}, returnMorph=True):\n",
    "    returnData = list()\n",
    "    \n",
    "    emojis = re.findall(':[_A-Za-z]+:',post)\n",
    "    for emoji in set(emojis):\n",
    "        emojiCounts = post.count(emoji)\n",
    "        post = post.replace(emoji,'')\n",
    "        returnData+=([(emoji,'EMJ')]*emojiCounts)\n",
    "        \n",
    "    hashTags = re.findall('#[_A-Za-z]',post)\n",
    "    for hashTag in set(hashTags):\n",
    "        hTagCounts = post.count(hashTag)\n",
    "        post = post.replace(hashTag,'')\n",
    "        returnData+=([(hashTag,'W_HASHTAG')]*hTagCounts)\n",
    "\n",
    "    \n",
    "    engChunks = re.findall('[A-Za-z]+[\\' ]?[A-Za-z]+',post)\n",
    "    for engChunk in set(engChunks):\n",
    "        chunkCounts = post.count(engChunk)\n",
    "        post = post.replace(engChunk,'')\n",
    "\n",
    "        targetToken = list()\n",
    "        for token in nltk.pos_tag(nltk.word_tokenize(engChunk)):\n",
    "            if token[1] in tagRule: \n",
    "                targetToken.append((token[0],token[1]))\n",
    "        returnData+=(targetToken*chunkCounts)\n",
    "\n",
    "    filterData = list()\n",
    "    for token in returnData:\n",
    "        if token[1] in tagRule:\n",
    "            if returnMorph==True:\n",
    "                filterData.append((token[0],tagRule[token[1]]))\n",
    "            else:\n",
    "                filterData.append(token[0])\n",
    "        \n",
    "    return filterData, post\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3722e6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('월드컵_1203.txt', 'r',encoding='utf-8') as f:\n",
    "    plaintext = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba7ee972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119 개의 데이터가 삭제되었습니다.\n"
     ]
    }
   ],
   "source": [
    "a = wordcloud(plaintext, 'test.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7026e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
