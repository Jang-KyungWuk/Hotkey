{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d041acbf",
   "metadata": {},
   "source": [
    "# 전처리 이용하지 않는, perplexity 최적화 토픽 수 찾기\n",
    "# 리턴값 = [[토픽1의 토큰들], [토픽2의 토큰들], ... ]\n",
    "# 작동 확인 시 순서대로 실행!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3f15d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from kiwipiepy import Kiwi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d255f35",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sklda(plaintext, n_top_words=5, n_iter=30):\n",
    "    \"\"\"\n",
    "    ------------------------------------------------------------------------------\n",
    "    \n",
    "    텍스트를 받아 lda로 토픽을 나눕니다.\n",
    "    토픽 수 별로 perplexity를 계산 한 후, perplexity값이 가장 낮은 토픽 수로 분석한 결과를 리스트로 리턴합니다.\n",
    "    \n",
    "    ------------------------------------------------------------------------------\n",
    "    \n",
    "    파라미터 설명\n",
    "    \n",
    "    plaintext : txt, 인스타그램 포스트들이 수집된 원문 텍스트. 'HOTKEY123!@#'로 포스트들을 구분한다.\n",
    "    n_top_words : int, 각 토픽 별로 상위 몇 개의 단어를 리턴할 지\n",
    "    n_iter : int, lda 분석 반복 수\n",
    "    \n",
    "    ------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "# 형태소분석기 키위 인스턴스 생성\n",
    "    kiwi = Kiwi()\n",
    "    kiwi.prepare()\n",
    "      \n",
    "# 인스타그램의 각 post들로 구성된 list 생성\n",
    "    doc = plaintext.replace('#','').split('HOTKEY123!@')\n",
    "# 전처리함수에서 원문반환이 될 경우 아래로 변경(구분자, 파라미터 등은 함수에 맞춰 변경필요)\n",
    "# doc = preprocess(plaintext, sep='HOTKEY123!@#', 원문반환)\n",
    "\n",
    "    print(\"\\nExtracting kiwi features for LDA...\")\n",
    "    t0 = time()\n",
    "    t1 = time()\n",
    "\n",
    "# sklearn CountVectorizer의 tokenizer 변수에 넣을 함수 정의\n",
    "    def tokenize_ko(doc):\n",
    "        tokens = kiwi.tokenize(doc)\n",
    "#         tagset = {'VA-I',  'MAG', 'XR', 'NNP', 'NNG'} <- 추가로 사용해 볼 만 한 태그들\n",
    "        tagset = {'NNP', 'NNG'}\n",
    "        results = []\n",
    "        for token in tokens:\n",
    "            if token.tag in tagset:\n",
    "                results.append(token.form)\n",
    "        return results\n",
    "\n",
    "# sklearn CountVectorizer를 통한 전처리\n",
    "    kiwi_vectorizer = CountVectorizer(min_df=2, max_features=1000, tokenizer=tokenize_ko)\n",
    "    kiwivoca = kiwi_vectorizer.fit_transform(doc)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# sklearn lda 분석을 통해 2~5개의 토픽 수 중 perplexity가 가장 낮은 값 찾기\n",
    "    print(\"\\nFinding the optimal number of topics...\")\n",
    "    t0 = time()\n",
    "    perplexity = []\n",
    "    for i in range(2,6):\n",
    "        lda = LatentDirichletAllocation(\n",
    "            n_components=i,\n",
    "            max_iter=n_iter,\n",
    "            learning_method=\"online\",\n",
    "            learning_offset=50.0,\n",
    "            random_state=0,\n",
    "        )\n",
    "        lda.fit(kiwivoca)\n",
    "        perplexity.append(lda.perplexity(kiwivoca))\n",
    "        \n",
    "# 가장 낮은 perplexity 값을 토픽 수로 저장\n",
    "    n_topics=perplexity.index(min(perplexity))+2\n",
    "    print(\"done in %0.3fs.\" % (time() - t0), f\"the optimal number of topics is {n_topics}\")\n",
    "\n",
    "# 가장 낮은 perplexity 값을 가지는 토픽 수로 lda분석\n",
    "    print(\"\\nFitting LDA models with KIWI features, number of topics=%d, max_iter=%d\" % (n_topics, n_iter))\n",
    "    t0 = time()\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics,\n",
    "        max_iter=n_iter,\n",
    "        learning_method=\"online\",\n",
    "        learning_offset=50.0,\n",
    "        random_state=0,\n",
    "    )\n",
    "    lda.fit(kiwivoca)\n",
    "\n",
    "# 토픽 수 : 토큰들 의 형태로 출력, 같은 토픽의 토큰들로 구성된 리스트 생성\n",
    "    kiwi_feature_names = kiwi_vectorizer.get_feature_names_out()\n",
    "    topic_list = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [kiwi_feature_names[i] for i in top_features_ind]\n",
    "        topic_list.append(top_features)\n",
    "        print('Topic {}: {}'.format(topic_idx+1, ' '.join(top_features))) \n",
    "        \n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    print(\"in total, %0.3fs.\" % (time() - t1))\n",
    "    return topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "637e0ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('월드컵_1203.txt','r',encoding='utf-8') as file:\n",
    "    plaintext = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3f051b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting kiwi features for LDA...\n",
      "done in 1.109s.\n",
      "\n",
      "Finding the optimal number of topics...\n",
      "done in 4.603s. the optimal number of topics is 2\n",
      "\n",
      "Fitting LDA models with KIWI features, number of topics=2, max_iter=30\n",
      "Topic 1: 월드컵 대한민국 강 진출 축구\n",
      "Topic 2: 스타 그램 월드컵 유머 대한민국\n",
      "done in 0.959s.\n",
      "in total, 6.671s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['월드컵', '대한민국', '강', '진출', '축구'], ['스타', '그램', '월드컵', '유머', '대한민국']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklda(plaintext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496b2626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
