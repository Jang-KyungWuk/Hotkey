{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d041acbf",
   "metadata": {},
   "source": [
    "# lda 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3827f1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from kiwipiepy import Kiwi\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "074bb78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklda(plaintext, n_top_words=300, n_iter=30):\n",
    "    \"\"\"\n",
    "    ------------------------------------------------------------------------------\n",
    "    \n",
    "    텍스트를 받아 lda로 토픽을 나눕니다.\n",
    "    토픽 수 별로 perplexity를 계산 한 후, perplexity값이 가장 낮은 토픽 수로 분석한 결과를 리스트로 리턴합니다.\n",
    "    각 토픽 별 워드클라우드 이미지를 생성합니다.\n",
    "    \n",
    "    ------------------------------------------------------------------------------\n",
    "    \n",
    "    파라미터 설명\n",
    "    \n",
    "    plaintext : txt, 인스타그램 포스트들이 수집된 원문 텍스트. 'HOTKEY123!@#'로 포스트들을 구분한다.\n",
    "    n_top_words : int, 각 토픽 별로 상위 몇 개의 단어를 리턴할 지\n",
    "    n_iter : int, lda 분석 반복 수\n",
    "    \n",
    "    ------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    \n",
    "# 형태소분석기 키위 인스턴스 생성\n",
    "    kiwi = Kiwi()\n",
    "    kiwi.prepare()\n",
    "      \n",
    "\n",
    "# 전처리함수를 통해 스팸포스트 제거\n",
    "    print(\"\\nFiltering spam post...\")\n",
    "    t0 = time()\n",
    "    t1 = time()\n",
    "    doc = preprocess(plaintext, sep='HOTKEY123!@#', returnPlain=True).replace('#','').split('HOTKEY123!@')\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    print(\"\\nExtracting kiwi features for LDA...\")\n",
    "    t0 = time()\n",
    "\n",
    "# sklearn CountVectorizer의 tokenizer 변수에 넣을 함수 정의\n",
    "    def tokenize_ko(doc):\n",
    "        tokens = kiwi.tokenize(doc)\n",
    "#         추가로 사용해 볼 만한 태그들\n",
    "        tagset = {'VA-I',  'MAG', 'XR', 'NNP', 'NNG'}\n",
    "#         tagset = {'NNP', 'NNG'}\n",
    "        results = []\n",
    "        for token in tokens:\n",
    "            if token.tag in tagset:\n",
    "                results.append(token.form)\n",
    "        return results\n",
    "\n",
    "# sklearn CountVectorizer를 통한 전처리\n",
    "    kiwi_vectorizer = CountVectorizer(min_df=2, max_features=1000, tokenizer=tokenize_ko)\n",
    "    kiwivoca = kiwi_vectorizer.fit_transform(doc)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# sklearn lda 분석을 통해 2~5개의 토픽 수 중 perplexity가 가장 낮은 값 찾기\n",
    "    print(\"\\nFinding the optimal number of topics...\")\n",
    "    t0 = time()\n",
    "    perplexity = []\n",
    "    for i in range(2,6):\n",
    "        lda = LatentDirichletAllocation(\n",
    "            n_components=i,\n",
    "            max_iter=n_iter,\n",
    "            learning_method=\"online\",\n",
    "            learning_offset=50.0,\n",
    "            random_state=0,\n",
    "        )\n",
    "        lda.fit(kiwivoca)\n",
    "        perplexity.append(lda.perplexity(kiwivoca))\n",
    "        \n",
    "# 가장 낮은 perplexity 값을 가지는 최적의 토픽 수로 저장\n",
    "    n_topics=perplexity.index(min(perplexity))+2\n",
    "    print(perplexity)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0), f\"the optimal number of topics is {n_topics}\")\n",
    "\n",
    "# 최적의 토픽 수로 lda분석\n",
    "    print(\"\\nFitting LDA models with KIWI features, number of topics=%d, max_iter=%d\" % (n_topics, n_iter))\n",
    "    t0 = time()\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics,\n",
    "        max_iter=n_iter,\n",
    "        learning_method=\"online\",\n",
    "        learning_offset=50.0,\n",
    "        random_state=0,\n",
    "    )\n",
    "    lda.fit(kiwivoca)\n",
    "\n",
    "# 토픽 넘버 : 해당 토픽의 토큰들 의 형태로 출력, 같은 토픽의 토큰들로 구성된 리스트 생성\n",
    "    kiwi_feature_names = kiwi_vectorizer.get_feature_names_out()\n",
    "    topic_list = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [kiwi_feature_names[i] for i in top_features_ind]\n",
    "        topic_list.append(top_features)\n",
    "        print('Topic {}: {}'.format(topic_idx+1, ' '.join(top_features))) \n",
    "        \n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "    \n",
    "    print(\"\\nGenerating word cloud for each topics\")\n",
    "    t0 = time()\n",
    "    \n",
    "# 워드클라우드 생성 수 찾기\n",
    "\n",
    "    if perplexity[0]>perplexity[2]:\n",
    "        lda = LatentDirichletAllocation(\n",
    "            n_components=4,\n",
    "            max_iter=n_iter,\n",
    "            learning_method=\"online\",\n",
    "            learning_offset=50.0,\n",
    "            random_state=0,\n",
    "        )\n",
    "    else:\n",
    "        lda = LatentDirichletAllocation(\n",
    "            n_components=2,\n",
    "            max_iter=n_iter,\n",
    "            learning_method=\"online\",\n",
    "            learning_offset=50.0,\n",
    "            random_state=0,\n",
    "        )\n",
    "    lda.fit(kiwivoca)\n",
    "        \n",
    "    \n",
    "# 워드클라우드 마스크 이미지 생성\n",
    "\n",
    "    im1 = Image.open('mask3.png') \n",
    "    im2 = Image.open('mask4.png')\n",
    "    im3 = Image.open('mask5.png')\n",
    "    im4 = Image.open('mask7.png')\n",
    "    \n",
    "# 이미지 파일 전처리\n",
    "    mask1=Image.new(\"RGB\",im1.size, (255,255,255))\n",
    "    mask1.paste(im1)\n",
    "#     mask1 = mask1.resize((500, 400))\n",
    "    mask1=np.array(mask1)\n",
    "    \n",
    "    mask2=Image.new(\"RGB\",im2.size, (255,255,255))\n",
    "    mask2.paste(im2)\n",
    "#     mask2 = mask2.resize((500, 400))\n",
    "    mask2=np.array(mask2)\n",
    "    \n",
    "    mask3=Image.new(\"RGB\",im3.size, (255,255,255))\n",
    "    mask3.paste(im3)\n",
    "#     mask3 = mask3.resize((500, 400))\n",
    "    mask3=np.array(mask3)\n",
    "    \n",
    "    mask4=Image.new(\"RGB\",im4.size, (255,255,255))\n",
    "    mask4.paste(im4)\n",
    "#     mask4 = mask4.resize((500, 400))\n",
    "    mask4=np.array(mask4)\n",
    "    \n",
    "    mask=[]\n",
    "    mask.append(mask1)\n",
    "    mask.append(mask2)\n",
    "    mask.append(mask3)\n",
    "    mask.append(mask4)\n",
    "    \n",
    "# 워드클라우드 생성\n",
    "    terms = kiwi_vectorizer.get_feature_names()\n",
    "    terms_count = 200\n",
    "    \n",
    "    for idx,topic in enumerate(lda.components_):    \n",
    "        print('Topic# ',idx+1)\n",
    "        abs_topic = abs(topic)\n",
    "        topic_terms = [[terms[i],topic[i]] for i in abs_topic.argsort()[:-terms_count-1:-1]]\n",
    "        topic_terms_sorted = [[terms[i], topic[i]] for i in abs_topic.argsort()[:-terms_count - 1:-1]]\n",
    "        topic_words = []\n",
    "        for i in range(terms_count):\n",
    "            topic_words.append(topic_terms_sorted[i][0])\n",
    "        print(','.join( word for word in topic_words))\n",
    "        print(\"\")\n",
    "        dict_word_frequency = {}\n",
    "\n",
    "        for i in range(terms_count):\n",
    "            dict_word_frequency[topic_terms_sorted[i][0]] = topic_terms_sorted[i][1]    \n",
    "        wc = WordCloud(background_color=\"white\",colormap='autumn',mask=mask[idx], max_words=100,\\\n",
    "                            max_font_size=60,min_font_size=10,prefer_horizontal=0.9, font_path ='NanumGothic.ttf')\n",
    "        wc.generate_from_frequencies(dict_word_frequency)       \n",
    "        wc.to_file(filename=f'Topic#{idx+1}.png')\n",
    "    \n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "    print(\"in total, %0.3fs.\" % (time() - t1))\n",
    "    \n",
    "    return topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be5a051",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
