{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9d6918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "from math import log1p\n",
    "import numpy as np\n",
    "import konlpy\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e51865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(plaintext, sep,\n",
    "               morphemeAnalyzer='kiwi', targetMorphs=['NNP','NNG'],removehashtag=True, returnMorph=False,\n",
    "               returnEnglishMorph=False, eeTagRule={'NNP':'NNP'},\n",
    "               filterMorphemeAnalyzer='kiwi',filterTargetMorphs=['NNP','NNG','W_HASHTAG'],\n",
    "               k_1Filter=1.5 ,bFilter=0.75,\n",
    "               filterThreshold = 3.315):\n",
    "    \n",
    "    # 형태소 분석기를 텍스트로 정의 시 해당 형태소 분석기를 할당\n",
    "    tma = set_morpheme_analyzer(morphemeAnalyzer)\n",
    "    fma = set_morpheme_analyzer(filterMorphemeAnalyzer)\n",
    "    \n",
    "    # 서버에서 \"글1 구분자 글2 구분자\" 형식의 데이터를 받는다고 가정\n",
    "    data = plaintext.split(sep)\n",
    "    # 구분자로 데이터를 받으면 글이 존재하지 않는 마지막 부분을 삭제\n",
    "    \n",
    "    if data[-1] in ('',' ','\\n',[],['\\n'],[' ']):\n",
    "        data=data[:-1]\n",
    "    \n",
    "    if removehashtag == True:\n",
    "        if '#' in sep:\n",
    "            sep = sep.replace('#','')\n",
    "        tdata = plaintext.replace('#',' ').split(sep)\n",
    "        if tdata[-1] in ('',' ','\\n',[],['\\n'],[' ']):\n",
    "            tdata=tdata[:-1]\n",
    "    else:\n",
    "        tdata = data*1\n",
    "            \n",
    "    postLens = list()\n",
    "    for post in data:\n",
    "        postLens.append(len(post))\n",
    "\n",
    "    flag = (morphemeAnalyzer==filterMorphemeAnalyzer, set(targetMorphs)==set(filterTargetMorphs))\n",
    "    \n",
    "    \n",
    "    ftok = data_tokenize(data,fma,filterTargetMorphs,\n",
    "                         returnMorph=False,\n",
    "                         returnEnglishMorph=True,\n",
    "                         eeTagRule={'W_HASHTAG':'W_HASHTAG'})\n",
    "    ttok = data_tokenize(data,tma,targetMorphs,\n",
    "                         returnMorph=returnMorph,\n",
    "                         returnEnglishMorph=returnEnglishMorph, eeTagRule=eeTagRule)\n",
    "    \n",
    "    filterScores = BM25(ftok, postLens, k_1=k_1Filter, b=bFilter)\n",
    "    \n",
    "    spamCount=0\n",
    "    for idx in range(len(filterScores)):\n",
    "        if filterScores[idx] < filterThreshold:\n",
    "            spamCount+=1\n",
    "            ttok.pop(idx-spamCount)\n",
    "    print(\"%s 개의 데이터가 삭제되었습니다.\"%spamCount)\n",
    "    \n",
    "    return ttok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2337aeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_morpheme_analyzer(maText):\n",
    "    if maText in ['kiwi','Kiwi','KIWI','키위']:\n",
    "        return Kiwi()\n",
    "    elif maText in ['Hannanum', 'hannanum', 'HANNANUM','한나눔']:\n",
    "        return konlpy.tag.Hannanum()\n",
    "    elif maText in ['Komoran','KOMORAN','komoran','코모란']:\n",
    "        return konlpy.tag.Komoran()\n",
    "    elif maText in ['Kkma','KKMA','kkma','꼬꼬마']:\n",
    "        return konlpy.tag.Kkma()\n",
    "    elif maText in ['Okt','OKT','okt','오픈코리안텍스트','트위터']:\n",
    "        return konlpy.tag.Okt()\n",
    "    elif maText in ['Mecab','mecab','MECAB','미캐브']:\n",
    "        return konlpy.tag.Mecab()\n",
    "    else:\n",
    "        raise Exception('No such morpheme analyzer\\nSupported morpheme analyzers are Kiwi, KoNLPy(Hannanum, Komoran, Kkma, Okt, Mecab)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ce5ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_tokenize(data,morphemeAnalyzer,targetMorph,\n",
    "                  returnMorph=False,\n",
    "                  returnEnglishMorph=False,\n",
    "                  eeTagRule={'NNP':'NNP'}):\n",
    "    \n",
    "    returnData = list()\n",
    "    \n",
    "    maDir = dir(morphemeAnalyzer)\n",
    "    if 'pos' in maDir:\n",
    "        for post in data:\n",
    "            selected = list()\n",
    "            if returnEnglishMorph==True:\n",
    "                eeTags, post = emoji_english_preprocess(post, tagRule=eeTagRule, returnMorph=returnMorph)\n",
    "                selected += eeTags\n",
    "            \n",
    "            for tok in morphemeAnalyzer.pos(post):\n",
    "                if tok[1] in targetMorph:\n",
    "                    if returnMorph==True:\n",
    "                        selected.append((tok[0],tok[1]))\n",
    "                    else:\n",
    "                        selected.append(tok[0])\n",
    "            returnData.append(selected)\n",
    "    \n",
    "    elif 'tokenize' in maDir:\n",
    "        for post in data:\n",
    "            selected = list()\n",
    "            if returnEnglishMorph==True:\n",
    "                eeTags, post = emoji_english_preprocess(post, tagRule=eeTagRule, returnMorph=returnMorph)\n",
    "                selected += eeTags\n",
    "            \n",
    "            for tok in morphemeAnalyzer.tokenize(post):\n",
    "                if tok.tag in targetMorph:\n",
    "                    if returnMorph==True:\n",
    "                        selected.append((tok.form,tok.tag))\n",
    "                    else:\n",
    "                        selected.append(tok.form)\n",
    "            returnData.append(selected)\n",
    "    else:\n",
    "        raise Exception('Not supported morpheme analyzer instance')\n",
    "    return returnData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a6b4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25(data, postLens, k_1=1.5, b=0.75):\n",
    "    avgPostLen = np.mean(postLens)\n",
    "    \n",
    "    N = len(data)\n",
    "    \n",
    "    n = dict()\n",
    "    for post in data:\n",
    "        uniqueToks = set(post)\n",
    "        for tok in uniqueToks:\n",
    "            try:\n",
    "                n[tok]+=1\n",
    "            except:\n",
    "                n[tok] = 1\n",
    "    \n",
    "    IDF = dict()\n",
    "    for tok in n.keys():\n",
    "        IDF[tok] = log1p((N-n[tok]+0.5)/(n[tok]+0.5))\n",
    "\n",
    "\n",
    "    filterScores = list()\n",
    "\n",
    "    for postidx, post in enumerate(data):\n",
    "        postScore = 0\n",
    "        for tok in post:\n",
    "            tokCount = post.count(tok)\n",
    "            postScore += (IDF[tok] * (\n",
    "                (tokCount*(k_1+1))/(\n",
    "                    tokCount+(k_1*(1-b+(b*(postLens[postidx]/avgPostLen)))))))\n",
    "        try:\n",
    "            filterScores.append((postScore/len(post)))\n",
    "        except:\n",
    "            filterScores.append(0)\n",
    "\n",
    "    return filterScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f562eda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_english_preprocess(post, tagRule={'NNP':'NNP'}, returnMorph=True):\n",
    "    returnData = list()\n",
    "    \n",
    "    emojis = re.findall(':[_A-Za-z]+:',post)\n",
    "    for emoji in set(emojis):\n",
    "        emojiCounts = post.count(emoji)\n",
    "        post = post.replace(emoji,'')\n",
    "        returnData+=([(emoji,'EMJ')]*emojiCounts)\n",
    "        \n",
    "    hashTags = re.findall('#[_A-Za-z]',post)\n",
    "    for hashTag in set(hashTags):\n",
    "        hTagCounts = post.count(hashTag)\n",
    "        post = post.replace(hashTag,'')\n",
    "        returnData+=([(hashTag,'W_HASHTAG')]*hTagCounts)\n",
    "\n",
    "    \n",
    "    engChunks = re.findall('[A-Za-z]+[\\' ]?[A-Za-z]+',post)\n",
    "    for engChunk in set(engChunks):\n",
    "        chunkCounts = post.count(engChunk)\n",
    "        post = post.replace(engChunk,'')\n",
    "\n",
    "        targetToken = list()\n",
    "        for token in nltk.pos_tag(nltk.word_tokenize(engChunk)):\n",
    "            if token[1] in tagRule: \n",
    "                targetToken.append((token[0],token[1]))\n",
    "        returnData+=(targetToken*chunkCounts)\n",
    "\n",
    "    filterData = list()\n",
    "    for token in returnData:\n",
    "        if token[1] in tagRule:\n",
    "            if returnMorph==True:\n",
    "                filterData.append((token[0],tagRule[token[1]]))\n",
    "            else:\n",
    "                filterData.append(token[0])\n",
    "        \n",
    "    return filterData, post"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
