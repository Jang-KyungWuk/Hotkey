{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a845fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "from math import log1p\n",
    "import numpy as np\n",
    "import konlpy\n",
    "import nltk\n",
    "import re\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79062e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(plaintext, sep,\n",
    "               morphemeAnalyzer='kiwi',morphemeAnalyzerParams=None, targetMorphs=['NNP','NNG'],\n",
    "               returnEnglishMorph=False, EETagRule={'NNP':'NNP'},\n",
    "               removeHashTag=True, returnPlain=False ,returnMorph=False,\n",
    "               filterMorphemeAnalyzer='kiwi', filterMorphemeAnalyzerParams=None, filterTargetMorphs=['NNP','NNG','W_HASHTAG'],\n",
    "               filterEnglishMorph=False, filterEETagRule={'NNP':'NNP'},\n",
    "               k_1Filter=1.5 ,bFilter=0.75,filterThreshold = 3.315):\n",
    "    \n",
    "    '''\n",
    "    t- 로 시작하는 변수들은 target, 실제로 반환되는 데이터\n",
    "    f- 로 시작하는 변수들은 filter, 내부적으로 BM25를 통해 필터링을 할 때 사용되는 데이터\n",
    "    '''\n",
    "    \n",
    "    # 형태소 분석기 인스턴스 생성\n",
    "    tma = setMorphemeAnalyzer(morphemeAnalyzer, morphemeAnalyzerParams)\n",
    "    fma = setMorphemeAnalyzer(filterMorphemeAnalyzer, filterMorphemeAnalyzerParams)\n",
    "    \n",
    "    # 구분자가 마지막에도 붙어있어 data 마지막에 비어있는 포스트가 있을 경우 이를 제거\n",
    "    data = plaintext.split(sep)\n",
    "    if data[-1] == '':\n",
    "        data=data[:-1]\n",
    "    \n",
    "    # 해쉬태그를 구성하는 '#'을 제거하고 싶을 경우 이를 제거\n",
    "    # 구분자에도 '#'이 포함되어 있을 경우 이 또한 제외\n",
    "    if removeHashTag == True:\n",
    "        if '#' in sep:\n",
    "            newSep = sep.replace('#','')\n",
    "        tdata = plaintext.replace('#',' ').split(newSep)\n",
    "        if tdata[-1] == '':\n",
    "            tdata=tdata[:-1]\n",
    "            \n",
    "    # 해쉬태그 처리가 없으면 기존의 위의 data 변수를 복제하여 사용\n",
    "    else:\n",
    "        tdata = data*1\n",
    "    \n",
    "    # BM25에서 사용하기 위한 원문서들의 길이를 저장\n",
    "    postLens = list()\n",
    "    for post in data:\n",
    "        postLens.append(len(post))\n",
    "        \n",
    "        \n",
    "    ftok = data_tokenize(data,fma,filterTargetMorphs,\n",
    "                         returnMorph=False,returnEnglishMorph=True,eeTagRule={'W_HASHTAG':'W_HASHTAG'})\n",
    "    \n",
    "    # 만약 모든 결과 분석의 조건들이 필터 분석의 조건들과 일치하면 이전의 토큰화 결과를 그대로 사용함\n",
    "    if (removeHashTag==False,\n",
    "        morphemeAnalyzer==filterMorphemeAnalyzer,\n",
    "        morphemeAnalyzerParams==filterMorphemeAnalyzerParams,\n",
    "        targetMorphs==filterTargetMorphs,\n",
    "        returnMorph==False,\n",
    "        returnEnglishMorph==filterEnglishMorph,\n",
    "        EETagRule==filterEETagRule):\n",
    "        flag=True\n",
    "        \n",
    "    filterScores = BM25(ftok, postLens, k_1=k_1Filter, b=bFilter)\n",
    "    spamCount = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    if flag=True:\n",
    "        tdata = '''\n",
    "    \n",
    "    idx=0\n",
    "    dataLen = len(postLens)\n",
    "    \n",
    "    while idx<dataLen:\n",
    "        if filterScores[idx] < filterThreshold:\n",
    "            spamCount+=1\n",
    "            tdata.pop(idx)\n",
    "            filterScores.pop(idx)\n",
    "            idx-=1\n",
    "            dataLen-=1\n",
    "        idx+=1\n",
    "        \n",
    "    print(\"%s 개의 데이터가 삭제되었습니다.\"%spamCount)\n",
    "    \n",
    "    if returnPlain==True:\n",
    "        return sep.join(tdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c67b367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_tokenize(data,morphemeAnalyzer,\n",
    "                  targetMorphs=['NNP','NNG'],\n",
    "                  returnMorph=False,\n",
    "                  returnEnglishMorph=False,\n",
    "                  eeTagRule={'NLTK_NNP':'NNP','NLTK_NN'\n",
    "                             'R_W_HASHTAG':'W_HASHTAG'}):\n",
    "    \n",
    "    returnData = list()\n",
    "    \n",
    "    if returnEnglishMorph == True:\n",
    "        for post in data:\n",
    "            partialReturn = list()\n",
    "            tokenizedData=HEMEK_tokenize(post,morphemeAnalyzer,nltkMA())\n",
    "            \n",
    "            for tok in tokenizedData:\n",
    "                if tok[1] in eeTagRule:\n",
    "                    tok[1] = eeTagRule[tok[1]]\n",
    "                if tok[1] in targetMorphs:\n",
    "                    if returnMorph == True:\n",
    "                        partialReturn.append(tok)\n",
    "                    else:\n",
    "                        partialReturn.append(tok[0])\n",
    "            returnData.append(partialReturn)\n",
    "     \n",
    "    else:\n",
    "        for post in data:\n",
    "            partialReturn=list()\n",
    "            tokenizedData = morphemeAnalyzer(post)\n",
    "            for tok in tokenizedData:\n",
    "                if tok[1] in targetMorphs:\n",
    "                    if returnMorph == True:\n",
    "                        partialReturn.append(tok)\n",
    "                    else:\n",
    "                        partialReturn.append(tok[0])\n",
    "            returnData.append(partialReturn)\n",
    "    \n",
    "    return returnData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c813b6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_demojized_set():\n",
    "    return set(re.findall(\"'en': '(:[^:]+:)'\",str(emoji.EMOJI_DATA.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3f4678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nltkMA:\n",
    "    def __init__(self,\n",
    "                 morph_header='NLTK_',\n",
    "                 word_tokenize_language='english',\n",
    "                 word_tokenize_preserve_line=False,\n",
    "                 pos_tag_tagset=None,\n",
    "                 pos_tag_lang='eng'):\n",
    "        \n",
    "        self.morph_header = morph_header\n",
    "        self.word_tokenize_language = word_tokenize_language\n",
    "        self.word_tokenize_preserve_line = word_tokenize_preserve_line\n",
    "        self.pos_tag_tagset = pos_tag_tagset\n",
    "        self.pos_tag_lang = pos_tag_lang\n",
    "        \n",
    "    def __call__(self,text):\n",
    "        result = list()\n",
    "        for token in nltk.pos_tag(nltk.word_tokenize(text,\n",
    "                                                     language=self.word_tokenize_language,\n",
    "                                                     preserve_line=self.word_tokenize_preserve_line),\n",
    "                                  tagset=self.pos_tag_tagset,\n",
    "                                  lang=self.pos_tag_lang):\n",
    "            result.append([token[0],self.morph_header+token[1]])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b35c1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class setMorphemeAnalyzer:\n",
    "    def __init__(self, maText,maParamDict=None):\n",
    "        if maText in ['kiwi','Kiwi','KIWI','키위']:\n",
    "            if maParamDict==None:\n",
    "                self.ma = Kiwi().tokenize\n",
    "            else:\n",
    "                if 'num_workers' in maParamDict:\n",
    "                    num_workers = maParamDict['num_workers']\n",
    "                else:\n",
    "                    num_workers = None\n",
    "\n",
    "                if 'model_path' in maParamDict:\n",
    "                    model_path = maParamDict['model_path']\n",
    "                else:\n",
    "                    model_path = None\n",
    "\n",
    "                if 'options' in maParamDict:\n",
    "                    options = maParamDict['options']\n",
    "                else:\n",
    "                    options = None\n",
    "\n",
    "                if 'integrate_allomorph' in maParamDict:\n",
    "                    integrate_allomorph = maParamDict['integrate_allomorph']\n",
    "                else:\n",
    "                    integrate_allomorph = None\n",
    "\n",
    "                if 'load_default_dict' in maParamDict:\n",
    "                    load_default_dict = maParamDict['load_default_dict']\n",
    "                else:\n",
    "                    load_default_dict = None\n",
    "\n",
    "                if 'load_typo_dict' in maParamDict:\n",
    "                    load_typo_dict = maParamDict['load_typo_dict']\n",
    "                else:\n",
    "                    load_typo_dict = None,\n",
    "\n",
    "                if 'model_type' in maParamDict:\n",
    "                    model_type = maParamDict['model_type']\n",
    "                else:\n",
    "                    model_type = 'knlm',\n",
    "\n",
    "                if 'typos' in maParamDict:\n",
    "                    typos = maParamDict['typos']\n",
    "                else:\n",
    "                    typos = None,\n",
    "\n",
    "                if 'typo_cost_threshold' in maParamDict:\n",
    "                    typo_cost_threshold = maParamDict['typo_cost_threshold']\n",
    "                else:\n",
    "                    typo_cost_threshold = 2.5\n",
    "\n",
    "                self.ma = Kiwi(num_workers=num_workers,model_path=model_path,\n",
    "                               options=options,integrate_allomorph=integrate_allomorph,\n",
    "                               load_default_dict=load_default_dict,load_typo_dict=load_typo_dict,\n",
    "                               model_type=model_type, typos=typos, typo_cost_threshold=typo_cost_threshold).tokenize\n",
    "\n",
    "        elif maText in ['Hannanum', 'hannanum', 'HANNANUM','한나눔']:\n",
    "            if maParamDict==None:\n",
    "                self.ma = konlpy.tag.Hannanum().pos\n",
    "            else:\n",
    "                if 'jvmpath' in maParamDict:\n",
    "                    jvmpath = maParamDict['jvmpath']\n",
    "                else:\n",
    "                    jvmpath=None\n",
    "\n",
    "                if 'max_heap_size' in maParamDict:\n",
    "                    max_heap_size = maParamDict['max_heap_size']\n",
    "                else:\n",
    "                    max_heap_size=1024\n",
    "\n",
    "                self.ma = konlpy.tag.Hannanum(jvmpath=jvmpath, max_heap_size=max_heap_size).pos\n",
    "\n",
    "        elif maText in ['Komoran','KOMORAN','komoran','코모란']:\n",
    "            if maParamDict == None:\n",
    "                self.ma = konlpy.tag.Komoran().pos\n",
    "            else:\n",
    "                if 'jvmpath' in maParamDict:\n",
    "                    jvmpath = maParamDict['jvmpath']\n",
    "                else:\n",
    "                    jvmpath=None\n",
    "\n",
    "                if 'userdic' in maParamDict:\n",
    "                    userdic = maParamDict['userdic']\n",
    "                else:\n",
    "                    userdic=None\n",
    "\n",
    "                if 'modelpath' in maParamDict:\n",
    "                    modelpath = maParamDict['modelpath']\n",
    "                else:\n",
    "                    modelpath=None\n",
    "\n",
    "                if 'max_heap_size' in maParamDict:\n",
    "                    max_heap_size = maParamDict['max_heap_size']\n",
    "                else:\n",
    "                    max_heap_size=1024\n",
    "\n",
    "                self.ma = konlpy.tag.Komoran(jvmpath=jvmpath, userdic=userdic,\n",
    "                                             modelpath=modelpath, max_heap_size=max_heap_size).pos\n",
    "\n",
    "        elif maText in ['Kkma','KKMA','kkma','꼬꼬마']:\n",
    "            if maParamDict == None:\n",
    "                self.ma = konlpy.tag.Kkma().pos\n",
    "            else:\n",
    "                if 'jvmpath' in maParamDict:\n",
    "                    jvmpath = maParamDict['jvmpath']\n",
    "                else:\n",
    "                    jvmpath=None\n",
    "\n",
    "                if 'max_heap_size' in maParamDict:\n",
    "                    max_heap_size = maParamDict['max_heap_size']\n",
    "                else:\n",
    "                    max_heap_size=1024\n",
    "\n",
    "                self.ma = konlpy.tag.Kkma(jvmpath=jvmpath, max_heap_size=max_heap_size).pos\n",
    "\n",
    "\n",
    "        elif maText in ['Okt','OKT','okt','오픈코리안텍스트','트위터']:\n",
    "            if maParamDict == None:\n",
    "                self.ma = konlpy.tag.Okt().pos\n",
    "            else:\n",
    "                if 'jvmpath' in maParamDict:\n",
    "                    jvmpath = maParamDict['jvmpath']\n",
    "                else:\n",
    "                    jvmpath=None\n",
    "\n",
    "                if 'max_heap_size' in maParamDict:\n",
    "                    max_heap_size = maParamDict['max_heap_size']\n",
    "                else:\n",
    "                    max_heap_size=1024\n",
    "\n",
    "                self.ma = konlpy.tag.Okt(jvmpath=jvmpath, max_heap_size=max_heap_size).pos\n",
    "\n",
    "        elif maText in ['Mecab','mecab','MECAB','미캐브']:\n",
    "            if maParamDict == None:\n",
    "                self.ma = konlpy.tag.Mecab().pos\n",
    "            else:\n",
    "                if 'dicpath' in maParamDict:\n",
    "                    dicpath = maParamDict['dicpath']\n",
    "                else:\n",
    "                    dicpath='/usr/local/lib/mecab/dic/mecab-ko-dic'\n",
    "\n",
    "                self.ma = konlpy.tag.Mecab(dicpath=dicpath).pos\n",
    "\n",
    "        else:\n",
    "            raise Exception('No such morpheme analyzer\\nSupported morpheme analyzers are Kiwi, KoNLPy(Hannanum, Komoran, Kkma, Okt, Mecab)')\n",
    "\n",
    "    def __call__(self,text):\n",
    "        result = list()\n",
    "        for token in self.ma(text):\n",
    "            result.append([token[0],token[1]])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aa82feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HEMEK_tokenize(text,KRmorphemeAnalyzer,NKRmorphemeAnalyzer):\n",
    "    emojis = get_demojized_set()\n",
    "    \n",
    "    chunks = list()\n",
    "    prevEnd = 0\n",
    "    for found in re.finditer(':[^: ]+:',text):\n",
    "        if found.group() not in emojis:\n",
    "            continue\n",
    "        start = found.start()\n",
    "        end = found.end()\n",
    "        if prevEnd!=start:\n",
    "            chunks.append([text[prevEnd:start],'CHUNK'])\n",
    "        chunks.append([text[start:end],'R_W_EMJ'])\n",
    "        prevEnd = end\n",
    "    if prevEnd != len(text):\n",
    "        chunks.append([text[prevEnd:],'CHUNK'])\n",
    "\n",
    "    HEMc = list()\n",
    "    for chunk in chunks:\n",
    "        if chunk[1] == 'CHUNK':\n",
    "            text = chunk[0]\n",
    "            chunkResult = list()\n",
    "            foundDict=dict()\n",
    "            for found in re.finditer('[#][^#@ ]+|#$',text):\n",
    "                foundDict[found.start()] = (found.end(),'R_W_HASHTAG')\n",
    "            for found in re.finditer('[@][^#@ㄱ-ㅎ가-힣 ]+|@$',text):\n",
    "                foundDict[found.start()] = (found.end(),'R_W_MENTION')\n",
    "\n",
    "            starts = list(foundDict.keys())\n",
    "            starts.sort()\n",
    "\n",
    "            prevEnd = 0\n",
    "            for start in starts:\n",
    "                end = foundDict[start][0]\n",
    "                morph = foundDict[start][1] \n",
    "                if prevEnd != start:\n",
    "                    chunkResult.append([text[prevEnd:start],'CHUNK'])\n",
    "                chunkResult.append([text[start:end],morph])\n",
    "                prevEnd = end\n",
    "\n",
    "            if prevEnd != len(text):\n",
    "                chunkResult.append([text[prevEnd:],'CHUNK'])\n",
    "\n",
    "            HEMc+=chunkResult\n",
    "\n",
    "        else:\n",
    "            HEMc.append(chunk)\n",
    "            \n",
    "    cursor = 0\n",
    "    flag = False\n",
    "    lenHEMc = len(HEMc)\n",
    "    while cursor < lenHEMc:\n",
    "        if HEMc[cursor][1] in ('R_W_HASHTAG','R_W_MENTION'):\n",
    "            if flag==True:\n",
    "                for merge in range(mergeCount):\n",
    "                    HEMc[mergePos][0] += HEMc.pop(mergePos+1)[0]\n",
    "                cursor-=mergeCount\n",
    "                lenHEMc-=mergeCount\n",
    "\n",
    "            mergePos = cursor*1\n",
    "            mergeCount = 0\n",
    "            flag=True\n",
    "\n",
    "        elif HEMc[cursor][1] == 'R_W_EMJ':\n",
    "            if flag==True:\n",
    "                mergeCount+=1\n",
    "        else:\n",
    "            if flag==True:\n",
    "                for merge in range(mergeCount):\n",
    "                    HEMc[mergePos][0] += HEMc.pop(mergePos+1)[0]\n",
    "                cursor-=mergeCount\n",
    "                lenHEMc-=mergeCount\n",
    "                flag=False\n",
    "        cursor+=1\n",
    "\n",
    "    if flag==True:\n",
    "        for merge in range(mergeCount):\n",
    "            tok, morph = HEMc.pop(mergePos+1)\n",
    "            HEMc[mergePos][0] += tok\n",
    "        \n",
    "\n",
    "\n",
    "    HEMEK = list()\n",
    "    \n",
    "    for chunk in HEMc:\n",
    "        text = chunk[0]\n",
    "        chunkResult = list()\n",
    "        foundDict=dict()\n",
    "        if chunk[1] == 'CHUNK':\n",
    "            for found in re.finditer('[ㄱ-ㅎ가-힣0-9\\,\\.\\/\\\\\\;\\'\\[\\]\\`\\-\\=\\<\\>\\?\\:\\\"\\{\\}\\|\\~\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\_\\+\\\"\\' ]+',text):\n",
    "                foundDict[found.start()] = (found.end(),'KR_CHUNK')\n",
    "            \n",
    "            prevEnd=0\n",
    "            for start in foundDict:\n",
    "                end = foundDict[start][0]\n",
    "                morph = foundDict[start][1] \n",
    "                if prevEnd != start:\n",
    "                    chunkResult.append([text[prevEnd:start],'NKR_CHUNK'])\n",
    "                chunkResult.append([text[start:end],morph])\n",
    "                prevEnd = end\n",
    "            if prevEnd != len(text):\n",
    "                chunkResult.append([text[prevEnd:],'NKR_CHUNK'])\n",
    "            HEMEK+=chunkResult\n",
    "        \n",
    "        else:\n",
    "            HEMEK.append(chunk)\n",
    "    \n",
    "    result = []\n",
    "    for chunk in HEMEK:\n",
    "        text = chunk[0]\n",
    "        if re.fullmatch('[ ]+||[\\n]+',text):\n",
    "            continue\n",
    "        elif chunk[1] == 'KR_CHUNK':\n",
    "            for token in KRmorphemeAnalyzer(text):\n",
    "                result.append([token[0],token[1]])\n",
    "            \n",
    "        elif chunk[1] == 'NKR_CHUNK':\n",
    "            for token in NKRmorphemeAnalyzer(text):\n",
    "                result.append([token[0],token[1]])\n",
    "        else:\n",
    "            result.append(chunk)\n",
    "        \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f81fa5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25(data, postLens, k_1=1.5, b=0.75):\n",
    "    avgPostLen = np.mean(postLens)\n",
    "    \n",
    "    N = len(data)\n",
    "    \n",
    "    n = dict()\n",
    "    for post in data:\n",
    "        uniqueToks = set(post)\n",
    "        for tok in uniqueToks:\n",
    "            try:\n",
    "                n[tok]+=1\n",
    "            except:\n",
    "                n[tok] = 1\n",
    "    \n",
    "    IDF = dict()\n",
    "    for tok in n.keys():\n",
    "        IDF[tok] = log1p((N-n[tok]+0.5)/(n[tok]+0.5))\n",
    "\n",
    "\n",
    "    filterScores = list()\n",
    "\n",
    "    for postidx, post in enumerate(data):\n",
    "        postScore = 0\n",
    "        for tok in post:\n",
    "            tokCount = post.count(tok)\n",
    "            postScore += (IDF[tok] * (\n",
    "                (tokCount*(k_1+1))/(\n",
    "                    tokCount+(k_1*(1-b+(b*(postLens[postidx]/avgPostLen)))))))\n",
    "        try:\n",
    "            filterScores.append((postScore/len(post)))\n",
    "        except:\n",
    "            filterScores.append(0)\n",
    "\n",
    "    return filterScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "811a29c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_english_preprocess(post, tagRule={'NNP':'NNP'}, returnMorph=True):\n",
    "    returnData = list()\n",
    "    \n",
    "    emojis = re.findall(':[_A-Za-z]+:',post)\n",
    "    for emoji in set(emojis):\n",
    "        emojiCounts = post.count(emoji)\n",
    "        post = post.replace(emoji,'')\n",
    "        returnData+=([(emoji,'EMJ')]*emojiCounts)\n",
    "        \n",
    "    hashTags = re.findall('#[_A-Za-z]',post)\n",
    "    for hashTag in set(hashTags):\n",
    "        hTagCounts = post.count(hashTag)\n",
    "        post = post.replace(hashTag,'')\n",
    "        returnData+=([(hashTag,'W_HASHTAG')]*hTagCounts)\n",
    "\n",
    "    \n",
    "    engChunks = re.findall('[A-Za-z]+[\\' ]?[A-Za-z]+',post)\n",
    "    for engChunk in set(engChunks):\n",
    "        chunkCounts = post.count(engChunk)\n",
    "        post = post.replace(engChunk,'')\n",
    "\n",
    "        targetToken = list()\n",
    "        for token in nltk.pos_tag(nltk.word_tokenize(engChunk)):\n",
    "            if token[1] in tagRule: \n",
    "                targetToken.append((token[0],token[1]))\n",
    "        returnData+=(targetToken*chunkCounts)\n",
    "\n",
    "    filterData = list()\n",
    "    for token in returnData:\n",
    "        if token[1] in tagRule:\n",
    "            if returnMorph==True:\n",
    "                filterData.append((token[0],tagRule[token[1]]))\n",
    "            else:\n",
    "                filterData.append(token[0])\n",
    "        \n",
    "    return filterData, post"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
