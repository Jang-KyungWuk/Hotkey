{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a845fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Morpheme Analyze\n",
    "from kiwipiepy import Kiwi\n",
    "import konlpy\n",
    "import nltk\n",
    "\n",
    "# Preprocess\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "# BM25\n",
    "from math import log1p\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3f4678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nltkMA:\n",
    "    def __init__(self,\n",
    "                 morph_header='NLTK_',\n",
    "                 word_tokenize_language='english',\n",
    "                 word_tokenize_preserve_line=False,\n",
    "                 pos_tag_tagset=None,\n",
    "                 pos_tag_lang='eng'):\n",
    "        \"\"\"\n",
    "        create instance and set parameters\n",
    "        \"\"\"\n",
    "    \n",
    "        # nltk로 형태소 분석에 사용되는 패러미터들을 할당\n",
    "        self.morph_header = morph_header\n",
    "        self.word_tokenize_language = word_tokenize_language\n",
    "        self.word_tokenize_preserve_line = word_tokenize_preserve_line\n",
    "        self.pos_tag_tagset = pos_tag_tagset\n",
    "        self.pos_tag_lang = pos_tag_lang\n",
    "        \n",
    "    def __call__(self,text):\n",
    "        \"\"\"\n",
    "        nltk.pos_tag(nltk.word_tokenize(text input))\n",
    "        \"\"\"\n",
    "        result = list()\n",
    "        for token in nltk.pos_tag(nltk.word_tokenize(text,\n",
    "                                                     language=self.word_tokenize_language,\n",
    "                                                     preserve_line=self.word_tokenize_preserve_line),\n",
    "                                  tagset=self.pos_tag_tagset,\n",
    "                                  lang=self.pos_tag_lang):\n",
    "            result.append([token[0],self.morph_header+token[1]])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b35c1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class setMorphemeAnalyzer:\n",
    "    def __init__(self, maText,maParamDict=None):\n",
    "        \n",
    "        '''\n",
    "        maText 를 토대로 형태소 분석기의 종류를 구분\n",
    "        maParamDict에 해당 형태소 분석기의 패러미터로 넣을 수 있는 값이 있으면 해당 값을, 없으면 기본값을 설정\n",
    "        형태소 분석기의 tokenize (키위) / pos (KoNLPy 계열) 함수에 __call__을 통해 입력받을 수 있는 형태의 인스턴스를 반환\n",
    "        '''\n",
    "        if maText in ['kiwi','Kiwi','KIWI','키위']:\n",
    "            if maParamDict==None:\n",
    "                self.ma = Kiwi().tokenize\n",
    "            else:\n",
    "                if 'num_workers' in maParamDict:\n",
    "                    num_workers = maParamDict['num_workers']\n",
    "                else:\n",
    "                    num_workers = None\n",
    "\n",
    "                if 'model_path' in maParamDict:\n",
    "                    model_path = maParamDict['model_path']\n",
    "                else:\n",
    "                    model_path = None\n",
    "\n",
    "                if 'options' in maParamDict:\n",
    "                    options = maParamDict['options']\n",
    "                else:\n",
    "                    options = None\n",
    "\n",
    "                if 'integrate_allomorph' in maParamDict:\n",
    "                    integrate_allomorph = maParamDict['integrate_allomorph']\n",
    "                else:\n",
    "                    integrate_allomorph = None\n",
    "\n",
    "                if 'load_default_dict' in maParamDict:\n",
    "                    load_default_dict = maParamDict['load_default_dict']\n",
    "                else:\n",
    "                    load_default_dict = None\n",
    "\n",
    "                if 'load_typo_dict' in maParamDict:\n",
    "                    load_typo_dict = maParamDict['load_typo_dict']\n",
    "                else:\n",
    "                    load_typo_dict = None,\n",
    "\n",
    "                if 'model_type' in maParamDict:\n",
    "                    model_type = maParamDict['model_type']\n",
    "                else:\n",
    "                    model_type = 'knlm',\n",
    "\n",
    "                if 'typos' in maParamDict:\n",
    "                    typos = maParamDict['typos']\n",
    "                else:\n",
    "                    typos = None,\n",
    "\n",
    "                if 'typo_cost_threshold' in maParamDict:\n",
    "                    typo_cost_threshold = maParamDict['typo_cost_threshold']\n",
    "                else:\n",
    "                    typo_cost_threshold = 2.5\n",
    "\n",
    "                self.ma = Kiwi(num_workers=num_workers,model_path=model_path,\n",
    "                               options=options,integrate_allomorph=integrate_allomorph,\n",
    "                               load_default_dict=load_default_dict,load_typo_dict=load_typo_dict,\n",
    "                               model_type=model_type, typos=typos, typo_cost_threshold=typo_cost_threshold).tokenize\n",
    "\n",
    "        elif maText in ['Hannanum', 'hannanum', 'HANNANUM','한나눔']:\n",
    "            if maParamDict==None:\n",
    "                self.ma = konlpy.tag.Hannanum().pos\n",
    "            else:\n",
    "                if 'jvmpath' in maParamDict:\n",
    "                    jvmpath = maParamDict['jvmpath']\n",
    "                else:\n",
    "                    jvmpath=None\n",
    "\n",
    "                if 'max_heap_size' in maParamDict:\n",
    "                    max_heap_size = maParamDict['max_heap_size']\n",
    "                else:\n",
    "                    max_heap_size=1024\n",
    "\n",
    "                self.ma = konlpy.tag.Hannanum(jvmpath=jvmpath, max_heap_size=max_heap_size).pos\n",
    "\n",
    "        elif maText in ['Komoran','KOMORAN','komoran','코모란']:\n",
    "            if maParamDict == None:\n",
    "                self.ma = konlpy.tag.Komoran().pos\n",
    "            else:\n",
    "                if 'jvmpath' in maParamDict:\n",
    "                    jvmpath = maParamDict['jvmpath']\n",
    "                else:\n",
    "                    jvmpath=None\n",
    "\n",
    "                if 'userdic' in maParamDict:\n",
    "                    userdic = maParamDict['userdic']\n",
    "                else:\n",
    "                    userdic=None\n",
    "\n",
    "                if 'modelpath' in maParamDict:\n",
    "                    modelpath = maParamDict['modelpath']\n",
    "                else:\n",
    "                    modelpath=None\n",
    "\n",
    "                if 'max_heap_size' in maParamDict:\n",
    "                    max_heap_size = maParamDict['max_heap_size']\n",
    "                else:\n",
    "                    max_heap_size=1024\n",
    "\n",
    "                self.ma = konlpy.tag.Komoran(jvmpath=jvmpath, userdic=userdic,\n",
    "                                             modelpath=modelpath, max_heap_size=max_heap_size).pos\n",
    "\n",
    "        elif maText in ['Kkma','KKMA','kkma','꼬꼬마']:\n",
    "            if maParamDict == None:\n",
    "                self.ma = konlpy.tag.Kkma().pos\n",
    "            else:\n",
    "                if 'jvmpath' in maParamDict:\n",
    "                    jvmpath = maParamDict['jvmpath']\n",
    "                else:\n",
    "                    jvmpath=None\n",
    "\n",
    "                if 'max_heap_size' in maParamDict:\n",
    "                    max_heap_size = maParamDict['max_heap_size']\n",
    "                else:\n",
    "                    max_heap_size=1024\n",
    "\n",
    "                self.ma = konlpy.tag.Kkma(jvmpath=jvmpath, max_heap_size=max_heap_size).pos\n",
    "\n",
    "\n",
    "        elif maText in ['Okt','OKT','okt','오픈코리안텍스트','트위터']:\n",
    "            if maParamDict == None:\n",
    "                self.ma = konlpy.tag.Okt().pos\n",
    "            else:\n",
    "                if 'jvmpath' in maParamDict:\n",
    "                    jvmpath = maParamDict['jvmpath']\n",
    "                else:\n",
    "                    jvmpath=None\n",
    "\n",
    "                if 'max_heap_size' in maParamDict:\n",
    "                    max_heap_size = maParamDict['max_heap_size']\n",
    "                else:\n",
    "                    max_heap_size=1024\n",
    "\n",
    "                self.ma = konlpy.tag.Okt(jvmpath=jvmpath, max_heap_size=max_heap_size).pos\n",
    "\n",
    "        elif maText in ['Mecab','mecab','MECAB','미캐브']:\n",
    "            if maParamDict == None:\n",
    "                self.ma = konlpy.tag.Mecab().pos\n",
    "            else:\n",
    "                if 'dicpath' in maParamDict:\n",
    "                    dicpath = maParamDict['dicpath']\n",
    "                else:\n",
    "                    dicpath='/usr/local/lib/mecab/dic/mecab-ko-dic'\n",
    "\n",
    "                self.ma = konlpy.tag.Mecab(dicpath=dicpath).pos\n",
    "\n",
    "        else:\n",
    "            raise Exception('No such morpheme analyzer\\nSupported morpheme analyzers are Kiwi, KoNLPy(Hannanum, Komoran, Kkma, Okt, Mecab)')\n",
    "\n",
    "    def __call__(self,text):\n",
    "        \n",
    "        '''\n",
    "        기존의 (토큰,품사) 튜플들을 담은 리스트를 반환하는 구조 대신 [토큰, 품사] 리스트를 담은 리스트를 반환\n",
    "        '''\n",
    "        result = list()\n",
    "        for token in self.ma(text):\n",
    "            result.append([token[0],token[1]])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79062e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(plaintext, sep,\n",
    "               returnIndex=False, returnTopIndex=None, \n",
    "               returnPlain=False, returnMorph=False,\n",
    "               multiReturn = False,\n",
    "               removeHashTag=True,\n",
    "               morphemeAnalyzer='kiwi',morphemeAnalyzerParams=None, targetMorphs=['NNP','NNG'],\n",
    "               returnEnglishMorph=True, EETagRule={'NLTK_NNP':'NNP','NLTK_NN':'NNG','R_W_HASHTAG':'W_HASHTAG'},\n",
    "               filterMorphemeAnalyzer='kiwi', filterMorphemeAnalyzerParams=None, filterTargetMorphs=['NNP','NNG','W_HASHTAG'],\n",
    "               filterEnglishMorph=True, filterEETagRule={'NLTK_NNP':'NNP','NLTK_NN':'NNG','R_W_HASHTAG':'W_HASHTAG'},\n",
    "               k_1Filter=1.5 ,bFilter=0.75,filterThreshold = 3.315):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    t- 로 시작하는 변수들은 target, 실제로 반환되는 데이터\n",
    "    f- 로 시작하는 변수들은 filter, 내부적으로 BM25를 통해 필터링을 할 때 사용되는 데이터\n",
    "    '''\n",
    "    \n",
    "    # 형태소 분석기 인스턴스 생성\n",
    "    tma = setMorphemeAnalyzer(morphemeAnalyzer, morphemeAnalyzerParams)\n",
    "    fma = setMorphemeAnalyzer(filterMorphemeAnalyzer, filterMorphemeAnalyzerParams)\n",
    "    \n",
    "    # 구분자가 마지막에도 붙어있어 data 마지막에 비어있는 포스트가 있을 경우 이를 제거\n",
    "    data = plaintext.split(sep)\n",
    "    if data[-1] == '':\n",
    "        data=data[:-1]\n",
    "    \n",
    "    # 해쉬태그를 구성하는 '#'을 제거하고 싶을 경우 이를 제거\n",
    "    # 구분자에도 '#'이 포함되어 있을 경우 이 또한 제외\n",
    "    if removeHashTag == True:\n",
    "        if '#' in sep:\n",
    "            newSep = sep.replace('#','')\n",
    "        tdata = plaintext.replace('#',' ').split(newSep)\n",
    "        if tdata[-1] == '':\n",
    "            tdata=tdata[:-1]\n",
    "            \n",
    "    # 해쉬태그 처리가 없으면 기존의 위의 data 변수를 복제하여 사용\n",
    "    else:\n",
    "        tdata = data*1\n",
    "    \n",
    "    # BM25에서 사용하기 위한 원문서들의 길이를 저장\n",
    "    postLens = list()\n",
    "    for post in data:\n",
    "        postLens.append(len(post))\n",
    "        \n",
    "    \n",
    "    # BM25 필터링에 사용 될 토큰화 된 결과값을 저장\n",
    "    ftok = data_tokenize(data,fma,filterTargetMorphs,\n",
    "                         returnMorph=False,returnEnglishMorph=True,eeTagRule=filterEETagRule)\n",
    "    \n",
    "    flag=False\n",
    "    # 만약 모든 결과 분석의 조건들이 필터 분석의 조건들과 일치하면 이전의 토큰화 결과를 그대로 사용할 것\n",
    "    if (removeHashTag==False and\\\n",
    "        morphemeAnalyzer==filterMorphemeAnalyzer and\\\n",
    "        morphemeAnalyzerParams==filterMorphemeAnalyzerParams and\\\n",
    "        targetMorphs==filterTargetMorphs and\\\n",
    "        returnMorph==False and\\\n",
    "        returnEnglishMorph==filterEnglishMorph and\\\n",
    "        EETagRule==filterEETagRule):\n",
    "        flag=True\n",
    "    \n",
    "    \n",
    "    # BM25를 통해 각 토큰들의 점수를 계산하고 문서별로 평균을 낸 결과를 저장\n",
    "    filterScores = BM25(ftok, postLens, k_1=k_1Filter, b=bFilter)\n",
    "    # 문서의 개수를 저장\n",
    "    dataLen = len(postLens)\n",
    "    \n",
    "    # 단일 결과 반환\n",
    "    if multiReturn == False:\n",
    "        if returnIndex == True: # 인덱스들을 반환\n",
    "            if returnTopIndex == None or returnTopIndex >= dataLen: # 최신 인덱스들을 전체 (혹은 지정 개수가 전체보다 커서 전체를) 반환\n",
    "                idxs = list(range(dataLen))\n",
    "                spamCount = 0\n",
    "                for idx, score in enumerate(filterScores):\n",
    "                    if score < filterThreshold:\n",
    "                        idxs.remove(idx)\n",
    "                        spamCount+=1\n",
    "                print(\"%s 개의 데이터가 삭제되었습니다.\"%spamCount)\n",
    "                return idxs\n",
    "                \n",
    "            else: # 최신 인덱스들을 지정 개수만큼 반환\n",
    "                idxs = list()\n",
    "                idxsCount = 0\n",
    "                idx = 0\n",
    "                while idxsCount < returnTopIndex:\n",
    "                    if filterScores[idx] >= filterThreshold:\n",
    "                        idxs.append(idx)\n",
    "                        idxsCount+=1\n",
    "                    idx+=1\n",
    "                return idxs\n",
    "        \n",
    "        # 반환 데이터 선택\n",
    "        elif returnPlain==True: # 원문을 반환하는 경우\n",
    "            returnData = data\n",
    "                    \n",
    "        elif flag==True: # 결과 데이터가 필터 데이터와 동일해서 바로 처리가 가능한 경우\n",
    "            returnData = ftok\n",
    "        \n",
    "        else: # 새로 작업을 해야 하는 경우\n",
    "            returnData = tdata\n",
    "        \n",
    "        idx=0\n",
    "        spamCount = 0\n",
    "        while idx<dataLen:\n",
    "            if filterScores[idx] < filterThreshold:\n",
    "                spamCount+=1\n",
    "                returnData.pop(idx) # 반환 데이터에서 점수 기준에 부합하지 않은 값 제거\n",
    "                filterScores.pop(idx) # 점수 기준에 부합하지 않은 값 제거\n",
    "                idx-=1\n",
    "                dataLen-=1\n",
    "            idx+=1\n",
    "        \n",
    "        if returnPlain==True: # 원문 반환\n",
    "            print(\"%s 개의 데이터가 삭제되었습니다.\"%spamCount)\n",
    "            return sep.join(returnData)\n",
    "        \n",
    "        elif flag==True: # 필터에서와 동일 데이터 반환\n",
    "            print(\"%s 개의 데이터가 삭제되었습니다.\"%spamCount)\n",
    "            return returnData\n",
    "        \n",
    "        else: # 모두 아닐 경우 함수 시작 시 정의한 값들로 형태소 분석 시작 후 결과 반환\n",
    "            returnData = data_tokenize(returnData, tma, targetMorphs,\n",
    "                                       returnMorph= returnMorph,\n",
    "                                       returnEnglishMorph=returnEnglishMorph,\n",
    "                                       eeTagRule=EETagRule)\n",
    "            print(\"%s 개의 데이터가 삭제되었습니다.\"%spamCount)\n",
    "            return returnData\n",
    "\n",
    "\n",
    "        \n",
    "    else: # 복수 결과 반환\n",
    "        returnDatas = dict() # 데이터를 반환할 딕셔너리\n",
    "        \n",
    "        \n",
    "        idxs = list(range(dataLen))\n",
    "        popIdxs = list()\n",
    "        spamCount = 0\n",
    "        for idx, score in enumerate(filterScores):\n",
    "            if score < filterThreshold:\n",
    "                idxs.remove(idx)\n",
    "                popIdxs.append(idx-spamCount)\n",
    "                spamCount+=1\n",
    "        \n",
    "        if returnIndex == True:\n",
    "            returnDatas['index'] = idxs\n",
    "            if returnTopIndex!=None:\n",
    "                if returnTopIndex >= dataLen:\n",
    "                    returnTopIndex=dataLen\n",
    "                \n",
    "                returnDatas['topIndex'] = idxs[:returnTopIndex]\n",
    "                \n",
    "        if returnPlain==True:\n",
    "            \n",
    "            for idx in popIdxs:\n",
    "                data.pop(idx)\n",
    "            returnDatas['plain']=sep.join(data)\n",
    "            \n",
    "        \n",
    "        for idx in popIdxs:\n",
    "            tdata.pop(idx)\n",
    "                \n",
    "        returnDatas['tokenized'] = data_tokenize(tdata, tma, targetMorphs,\n",
    "                                                 returnMorph = returnMorph,\n",
    "                                                 returnEnglishMorph = returnEnglishMorph,\n",
    "                                                 eeTagRule = EETagRule)\n",
    "        print(\"%s 개의 데이터가 삭제되었습니다.\"%spamCount)\n",
    "        return returnDatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c67b367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_tokenize(data,morphemeAnalyzer,\n",
    "                  targetMorphs=['NNP','NNG'],\n",
    "                  returnMorph=False,\n",
    "                  returnEnglishMorph=False,\n",
    "                  eeTagRule={'NLTK_NNP':'NNP',\n",
    "                             'NLTK_NN':'NNG',\n",
    "                             'R_W_HASHTAG':'W_HASHTAG'}):\n",
    "    \n",
    "    returnData = list()\n",
    "    \n",
    "    if returnEnglishMorph == True:\n",
    "        for post in data:\n",
    "            partialReturn = list()\n",
    "            tokenizedData=HEMEK_tokenize(post,morphemeAnalyzer,nltkMA())\n",
    "            \n",
    "            for tok in tokenizedData:\n",
    "                if tok[1] in eeTagRule:\n",
    "                    tok[1] = eeTagRule[tok[1]]\n",
    "                if tok[1] in targetMorphs:\n",
    "                    if returnMorph == True:\n",
    "                        partialReturn.append(tok)\n",
    "                    else:\n",
    "                        partialReturn.append(tok[0])\n",
    "            returnData.append(partialReturn)\n",
    "     \n",
    "    else:\n",
    "        for post in data:\n",
    "            partialReturn=list()\n",
    "            tokenizedData = morphemeAnalyzer(post)\n",
    "            for tok in tokenizedData:\n",
    "                if tok[1] in targetMorphs:\n",
    "                    if returnMorph == True:\n",
    "                        partialReturn.append(tok)\n",
    "                    else:\n",
    "                        partialReturn.append(tok[0])\n",
    "            returnData.append(partialReturn)\n",
    "    \n",
    "    return returnData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02476dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stopwordRule={'\\n':' ','\\u200b':' ','\\\\n':' '}):\n",
    "    \n",
    "    '''\n",
    "    입력받은 텍스트를 stopwordRule 에 정의된 불용어:대체 텍스트 쌍대로 불용어를 대체 텍스트로 교체\n",
    "    '''\n",
    "    for stopword in stopwordRule:\n",
    "        text = text.replace(stopword,stopwordRule[stopword])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c813b6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_demojized_set():\n",
    "    \n",
    "    '''\n",
    "    emoji 라이브러리로 demojize 결과로 나올 수 있는 모든 :이모티콘 이름: 형식 set 에 담아 반환\n",
    "    '''\n",
    "    return set(re.findall(\"'en': '(:[^:]+:)'\",str(emoji.EMOJI_DATA.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b791a8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regexp_spliter(text, regexps, matchLabels, nomatchLabel, filters=None):\n",
    "    \n",
    "    '''\n",
    "    정규표현식들을 이용해 입력받은 텍스트들을 나누고 [나눠진 텍스트, 라벨] 리스트를 담은 리스트로 반환\n",
    "    \n",
    "    ex : ABCDEFG\n",
    "    입력 받은 정규 표현식 : BC, EF\n",
    "    입력 받은 라벨 : label1 label2\n",
    "    입력 받은 불일치 라벨 : NO\n",
    "    실행 결과 : [[A, NO], [BC, label1], [D, NO], [EF, label2], [G, NO]]\n",
    "    \n",
    "    정규표현식은 모두 re.finditer(정규표현식,입력텍스트) 으로 적용\n",
    "    '''\n",
    "    if type(regexps) == str:\n",
    "        regexps = [regexps]\n",
    "    if type(matchLabels) == str:\n",
    "        matchLabels = [matchLabels]\n",
    "    expCount = len(regexps)\n",
    "    \n",
    "    if filters == None:\n",
    "        filters = [None]*expCount\n",
    "\n",
    "    if expCount != len(matchLabels) and expCount != len(filters):\n",
    "        raise Exception('Regular Expression and Label (and filter) counts are not matched')\n",
    "    \n",
    "    \n",
    "    returnData = list()\n",
    "    \n",
    "    foundDict=dict()\n",
    "    for idx in range(expCount):\n",
    "        for found in re.finditer(regexps[idx],text):\n",
    "            if filters[idx] == None:\n",
    "                pass\n",
    "            elif found.group() not in filters[idx]:\n",
    "                continue\n",
    "            \n",
    "            foundDict[found.start()] = (found.end(),matchLabels[idx])\n",
    "    \n",
    "    starts = list(foundDict.keys())\n",
    "    starts.sort()\n",
    "    prevEnd = 0\n",
    "    for start in starts:\n",
    "        end = foundDict[start][0]\n",
    "        label = foundDict[start][1]     \n",
    "        if prevEnd!=start:\n",
    "            returnData.append([text[prevEnd:start],nomatchLabel])\n",
    "        returnData.append([text[start:end],label])\n",
    "        prevEnd = end\n",
    "        \n",
    "    if prevEnd != len(text):\n",
    "        returnData.append([text[prevEnd:],nomatchLabel])\n",
    "    \n",
    "    return returnData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aa82feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HEMEK_tokenize(text,KRmorphemeAnalyzer,NKRmorphemeAnalyzer):\n",
    "    emojis = get_demojized_set()\n",
    "    \n",
    "    chunks = regexp_spliter(text,[':[^: ]+:'],['R_W_EMJ'],'CHUNK',[emojis])\n",
    "    \n",
    "    HEMc = list()\n",
    "    for chunk in chunks:\n",
    "        if chunk[1] == 'CHUNK':\n",
    "            HEMc+=regexp_spliter(chunk[0],['[#][^#@ ]+|#$','[@][^#@ㄱ-ㅎ가-힣 ]+|@$'],['R_W_HASHTAG','R_W_MENTION'],'CHUNK')\n",
    "        else:\n",
    "            HEMc.append(chunk)\n",
    "            \n",
    "    cursor = 0\n",
    "    flag = False\n",
    "    lenHEMc = len(HEMc)\n",
    "    while cursor < lenHEMc:\n",
    "        if HEMc[cursor][1] in ('R_W_HASHTAG','R_W_MENTION'):\n",
    "            if flag==True:\n",
    "                for merge in range(mergeCount):\n",
    "                    HEMc[mergePos][0] += HEMc.pop(mergePos+1)[0]\n",
    "                cursor-=mergeCount\n",
    "                lenHEMc-=mergeCount\n",
    "\n",
    "            mergePos = cursor*1\n",
    "            mergeCount = 0\n",
    "            flag=True\n",
    "\n",
    "        elif HEMc[cursor][1] == 'R_W_EMJ':\n",
    "            if flag==True:\n",
    "                mergeCount+=1\n",
    "        else:\n",
    "            if flag==True:\n",
    "                for merge in range(mergeCount):\n",
    "                    HEMc[mergePos][0] += HEMc.pop(mergePos+1)[0]\n",
    "                cursor-=mergeCount\n",
    "                lenHEMc-=mergeCount\n",
    "                flag=False\n",
    "        cursor+=1\n",
    "\n",
    "    if flag==True:\n",
    "        for merge in range(mergeCount):\n",
    "            tok, morph = HEMc.pop(mergePos+1)\n",
    "            HEMc[mergePos][0] += tok\n",
    "        \n",
    "\n",
    "    HEMEK = list()\n",
    "    for chunk in HEMc:\n",
    "        if chunk[1] == 'CHUNK':\n",
    "            HEMEK+=regexp_spliter(remove_stopwords(chunk[0]),\n",
    "                                  ['[ㄱ-ㅎ가-힣0-9\\,\\.\\/\\\\\\;\\'\\[\\]\\`\\-\\=\\<\\>\\?\\:\\\"\\{\\}\\|\\~\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\_\\+\\\"\\' ]+'],\n",
    "                                  ['KR_CHUNK'],'NKR_CHUNK')\n",
    "        else:\n",
    "            HEMEK.append(chunk)\n",
    "    \n",
    "    result = []\n",
    "    for chunk in HEMEK:\n",
    "        text = chunk[0]\n",
    "        if re.fullmatch('[ ]+||[\\n]+',text):\n",
    "            continue\n",
    "        elif chunk[1] == 'KR_CHUNK':\n",
    "            for token in KRmorphemeAnalyzer(text):\n",
    "                result.append([token[0],token[1]])\n",
    "            \n",
    "        elif chunk[1] == 'NKR_CHUNK':\n",
    "            for token in NKRmorphemeAnalyzer(text):\n",
    "                result.append([token[0],token[1]])\n",
    "        else:\n",
    "            result.append(chunk)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f81fa5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25(data, postLens, k_1=1.5, b=0.75):\n",
    "    avgPostLen = np.mean(postLens)\n",
    "    \n",
    "    N = len(data)\n",
    "    \n",
    "    n = dict()\n",
    "    for post in data:\n",
    "        uniqueToks = set(post)\n",
    "        for tok in uniqueToks:\n",
    "            try:\n",
    "                n[tok]+=1\n",
    "            except:\n",
    "                n[tok] = 1\n",
    "    \n",
    "    IDF = dict()\n",
    "    for tok in n.keys():\n",
    "        IDF[tok] = log1p((N-n[tok]+0.5)/(n[tok]+0.5))\n",
    "\n",
    "\n",
    "    filterScores = list()\n",
    "\n",
    "    for postidx, post in enumerate(data):\n",
    "        postScore = 0\n",
    "        for tok in post:\n",
    "            tokCount = post.count(tok)\n",
    "            postScore += (IDF[tok] * (\n",
    "                (tokCount*(k_1+1))/(\n",
    "                    tokCount+(k_1*(1-b+(b*(postLens[postidx]/avgPostLen)))))))\n",
    "        try:\n",
    "            filterScores.append((postScore/len(post)))\n",
    "        except:\n",
    "            filterScores.append(0)\n",
    "\n",
    "    return filterScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2094bf29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
